{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pima-indians-diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('pima-indians-diabetes.csv', \n",
    "                       names = ['pregnant', 'plasma','pressure','thickness','insulin','BMI','pedigress','age','class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>plasma</th>\n",
       "      <th>pressure</th>\n",
       "      <th>thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>pedigress</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  plasma  pressure  thickness  insulin   BMI  pedigress  age  class\n",
       "0         6     148        72         35        0  33.6      0.627   50      1\n",
       "1         1      85        66         29        0  26.6      0.351   31      0\n",
       "2         8     183        64          0        0  23.3      0.672   32      1\n",
       "3         1      89        66         23       94  28.1      0.167   21      0\n",
       "4         0     137        40         35      168  43.1      2.288   33      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "77/77 - 0s - loss: 1.1387 - accuracy: 0.6185\n",
      "Epoch 2/20\n",
      "77/77 - 0s - loss: 0.7410 - accuracy: 0.6146\n",
      "Epoch 3/20\n",
      "77/77 - 0s - loss: 0.7293 - accuracy: 0.6419\n",
      "Epoch 4/20\n",
      "77/77 - 0s - loss: 0.7582 - accuracy: 0.6198\n",
      "Epoch 5/20\n",
      "77/77 - 0s - loss: 0.6568 - accuracy: 0.6745\n",
      "Epoch 6/20\n",
      "77/77 - 0s - loss: 0.6337 - accuracy: 0.6797\n",
      "Epoch 7/20\n",
      "77/77 - 0s - loss: 0.6355 - accuracy: 0.6510\n",
      "Epoch 8/20\n",
      "77/77 - 0s - loss: 0.6221 - accuracy: 0.6771\n",
      "Epoch 9/20\n",
      "77/77 - 0s - loss: 0.6136 - accuracy: 0.6888\n",
      "Epoch 10/20\n",
      "77/77 - 0s - loss: 0.6256 - accuracy: 0.6862\n",
      "Epoch 11/20\n",
      "77/77 - 0s - loss: 0.5887 - accuracy: 0.7070\n",
      "Epoch 12/20\n",
      "77/77 - 0s - loss: 0.6267 - accuracy: 0.6953\n",
      "Epoch 13/20\n",
      "77/77 - 0s - loss: 0.5866 - accuracy: 0.7135\n",
      "Epoch 14/20\n",
      "77/77 - 0s - loss: 0.5752 - accuracy: 0.7148\n",
      "Epoch 15/20\n",
      "77/77 - 0s - loss: 0.5943 - accuracy: 0.7031\n",
      "Epoch 16/20\n",
      "77/77 - 0s - loss: 0.5906 - accuracy: 0.7031\n",
      "Epoch 17/20\n",
      "77/77 - 0s - loss: 0.5771 - accuracy: 0.7018\n",
      "Epoch 18/20\n",
      "77/77 - 0s - loss: 0.5766 - accuracy: 0.7083\n",
      "Epoch 19/20\n",
      "77/77 - 0s - loss: 0.5703 - accuracy: 0.7096\n",
      "Epoch 20/20\n",
      "77/77 - 0s - loss: 0.5608 - accuracy: 0.7135\n",
      "24/24 [==============================] - 0s 499us/step - loss: 0.5362 - accuracy: 0.7305\n",
      "\n",
      " Accuracy: 0.7305\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "x_data = dataset[:, 0:8]\n",
    "y_data = dataset[:, 8]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim = 8, activation  ='relu'))\n",
    "model.add(Dense(24, activation ='relu'))\n",
    "model.add(Dense(24, activation ='relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer= \"adam\",  metrics = [\"accuracy\"])\n",
    "# 이진 분류 문제에서는 binary-crossentropy를 사용한다\n",
    "\n",
    "\n",
    "model.fit(x_data, y_data, epochs = 20, verbose= 2 , batch_size = 10)\n",
    "\n",
    "print(\"\\n Accuracy: %.4f\" % model.evaluate(x_data, y_data)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 0s 677us/step - loss: 0.3469 - accuracy: 0.6523\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 0s 739us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 0s 800us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 0s 762us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 0s 732us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 0s 769us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 0s 744us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 0s 755us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 0s 754us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 0s 755us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 0s 744us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 0s 753us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 0s 726us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 0s 718us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 0s 771us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 0s 742us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 0s 770us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 0s 759us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 0s 728us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 0s 787us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 0s 756us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 0s 733us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 0s 749us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 0s 737us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 0s 757us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 0s 744us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 0s 768us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 0s 742us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 0s 763us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 0s 781us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 0s 735us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 0s 738us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 0s 752us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 0s 737us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 0s 752us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 0s 730us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 0s 732us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 0s 731us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 0s 739us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 0s 729us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 0s 749us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 0s 745us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 0s 755us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 0s 749us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 0s 738us/step - loss: 0.3488 - accuracy: 0.6510\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 0s 748us/step - loss: 0.3473 - accuracy: 0.6523\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 0s 754us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 0s 738us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 0s 744us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 0s 723us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 0s 744us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 0s 729us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 0s 757us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 0s 750us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 0s 745us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 0s 739us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 0s 736us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 0s 754us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 0s 728us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 0s 746us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 0s 733us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 0s 726us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 0s 743us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 0s 736us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 0s 730us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 0s 753us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 0s 732us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 0s 736us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 0s 740us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 0s 754us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 0s 717us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 0s 721us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 0s 753us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 0s 731us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 769us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 0s 740us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 0s 744us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 0s 730us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 0s 776us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 0s 720us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 0s 731us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 0s 756us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 0s 874us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 0s 738us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 0s 754us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 0s 741us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 0s 750us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 0s 740us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.3565 - accuracy: 0.64 - 0s 746us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 0s 758us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 0s 752us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 0s 761us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "24/24 [==============================] - 0s 498us/step - loss: 0.3490 - accuracy: 0.6510\n",
      "\n",
      " Accuracy: 0.6510\n"
     ]
    }
   ],
   "source": [
    "data_set = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter = \",\")\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "x_data = data_set[:,0:8] #몸무게\n",
    "y_data = data_set[:,8]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim = 8, activation ='relu'))\n",
    "model.add(Dense(50,activation ='relu'))\n",
    "model.add(Dense(30,activation ='relu'))\n",
    "#레이어 추가하기\n",
    "#Dense(n, ~~~ ) n은 마음대로 설정가능\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.fit(x_data, y_data, epochs = 100, verbose=1, batch_size = 10)\n",
    "\n",
    "#print(model.predict(x_data))\n",
    "print(\"\\n Accuracy: %.4f\" % model.evaluate(x_data, y_data)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 분류 문제\n",
    "------\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.csv', \n",
    "                       names = ['sepal_length', 'sepal_width','petal_length','petal_width','species'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from seaborn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from seaborn) (1.18.5)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from seaborn) (3.3.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from pandas>=0.22.0->seaborn) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from pandas>=0.22.0->seaborn) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.22.0->seaborn) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 823.25x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pairplot(df,hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder#원핫인코딩\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#seed값 설정\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed) #tf 2.0버전에서 이렇게 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "150/150 [==============================] - 0s 559us/step - loss: 1.6271 - accuracy: 0.5533\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 0s 564us/step - loss: 0.6383 - accuracy: 0.7000\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 0s 546us/step - loss: 0.5596 - accuracy: 0.7733\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 0s 544us/step - loss: 0.4948 - accuracy: 0.8200\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 0s 554us/step - loss: 0.4555 - accuracy: 0.8133\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 0s 546us/step - loss: 0.4224 - accuracy: 0.8867\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 0s 563us/step - loss: 0.3882 - accuracy: 0.9000\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 0s 539us/step - loss: 0.3627 - accuracy: 0.8933\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 0s 560us/step - loss: 0.3512 - accuracy: 0.9067\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 0s 559us/step - loss: 0.3256 - accuracy: 0.9733\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 0s 553us/step - loss: 0.3126 - accuracy: 0.9733\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 0s 536us/step - loss: 0.2983 - accuracy: 0.9533\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 0s 529us/step - loss: 0.2808 - accuracy: 0.9667\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 0s 541us/step - loss: 0.2651 - accuracy: 0.9667\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 0s 523us/step - loss: 0.2534 - accuracy: 0.9667\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 0s 545us/step - loss: 0.2436 - accuracy: 0.9667\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 0s 551us/step - loss: 0.2319 - accuracy: 0.9533\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 0s 554us/step - loss: 0.2308 - accuracy: 0.9400\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 0s 543us/step - loss: 0.2122 - accuracy: 0.9533\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 0s 540us/step - loss: 0.2071 - accuracy: 0.9733\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 0s 550us/step - loss: 0.1971 - accuracy: 0.9733\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 0s 540us/step - loss: 0.1886 - accuracy: 0.9800\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 0s 534us/step - loss: 0.1801 - accuracy: 0.9800\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 0s 558us/step - loss: 0.1734 - accuracy: 0.9800\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 0s 552us/step - loss: 0.1719 - accuracy: 0.9533\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 0s 544us/step - loss: 0.1610 - accuracy: 0.9667\n",
      "Epoch 27/50\n",
      "150/150 [==============================] - 0s 536us/step - loss: 0.1546 - accuracy: 0.9733\n",
      "Epoch 28/50\n",
      "150/150 [==============================] - 0s 536us/step - loss: 0.1519 - accuracy: 0.9733\n",
      "Epoch 29/50\n",
      "150/150 [==============================] - 0s 540us/step - loss: 0.1507 - accuracy: 0.9800\n",
      "Epoch 30/50\n",
      "150/150 [==============================] - 0s 550us/step - loss: 0.1378 - accuracy: 0.9600\n",
      "Epoch 31/50\n",
      "150/150 [==============================] - 0s 547us/step - loss: 0.1384 - accuracy: 0.9667\n",
      "Epoch 32/50\n",
      "150/150 [==============================] - 0s 539us/step - loss: 0.1314 - accuracy: 0.9800\n",
      "Epoch 33/50\n",
      "150/150 [==============================] - 0s 550us/step - loss: 0.1294 - accuracy: 0.9733\n",
      "Epoch 34/50\n",
      "150/150 [==============================] - 0s 549us/step - loss: 0.1260 - accuracy: 0.9733\n",
      "Epoch 35/50\n",
      "150/150 [==============================] - 0s 538us/step - loss: 0.1289 - accuracy: 0.9600\n",
      "Epoch 36/50\n",
      "150/150 [==============================] - 0s 540us/step - loss: 0.1129 - accuracy: 0.9867\n",
      "Epoch 37/50\n",
      "150/150 [==============================] - 0s 527us/step - loss: 0.1190 - accuracy: 0.9733\n",
      "Epoch 38/50\n",
      "150/150 [==============================] - 0s 526us/step - loss: 0.1203 - accuracy: 0.9733\n",
      "Epoch 39/50\n",
      "150/150 [==============================] - 0s 541us/step - loss: 0.1100 - accuracy: 0.9800\n",
      "Epoch 40/50\n",
      "150/150 [==============================] - 0s 540us/step - loss: 0.1131 - accuracy: 0.9733\n",
      "Epoch 41/50\n",
      "150/150 [==============================] - 0s 532us/step - loss: 0.1098 - accuracy: 0.9600\n",
      "Epoch 42/50\n",
      "150/150 [==============================] - 0s 546us/step - loss: 0.1052 - accuracy: 0.9867\n",
      "Epoch 43/50\n",
      "150/150 [==============================] - 0s 543us/step - loss: 0.1091 - accuracy: 0.9467\n",
      "Epoch 44/50\n",
      "150/150 [==============================] - 0s 530us/step - loss: 0.1140 - accuracy: 0.9667\n",
      "Epoch 45/50\n",
      "150/150 [==============================] - 0s 536us/step - loss: 0.1029 - accuracy: 0.9800\n",
      "Epoch 46/50\n",
      "150/150 [==============================] - 0s 551us/step - loss: 0.1001 - accuracy: 0.9667\n",
      "Epoch 47/50\n",
      "150/150 [==============================] - 0s 536us/step - loss: 0.1040 - accuracy: 0.9667\n",
      "Epoch 48/50\n",
      "150/150 [==============================] - 0s 554us/step - loss: 0.0965 - accuracy: 0.9800\n",
      "Epoch 49/50\n",
      "150/150 [==============================] - 0s 547us/step - loss: 0.0885 - accuracy: 0.9733\n",
      "Epoch 50/50\n",
      "150/150 [==============================] - 0s 532us/step - loss: 0.0895 - accuracy: 0.9667\n",
      "5/5 [==============================] - 0s 599us/step - loss: 0.0874 - accuracy: 0.9800\n",
      "\n",
      " Accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "#문자열을 불러올때는 원핫인코딩을 사용해야함\n",
    "\n",
    "#데이터분류\n",
    "dataset=df.values\n",
    "X=dataset[:,0:4].astype(float)\n",
    "Y_obj=dataset[:,4]\n",
    "\n",
    "#원핫인코딩\n",
    "#문자열을 숫자로 변환\n",
    "e=LabelEncoder()\n",
    "e.fit(Y_obj)\n",
    "Y=e.transform(Y_obj)\n",
    "Y_encoded=np_utils.to_categorical(Y)\n",
    "\n",
    "#모델 설정\n",
    "#최종 출력 값이 3개 중 하나여야 하므로 출력층에 해당하는 덴스를 3개로 만들어줌\n",
    "model=Sequential()\n",
    "model.add(Dense(16,input_dim=4,activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))#활성화함수\n",
    "#소프트맥스: 총합이 1인 형태로 바꿔서 계산해 주는 함수\n",
    "#합계가 1인 형태로 변환하면 큰 값이 두드러지게 나타나고 작은 값은 더 작아짐\n",
    "#이 값이 교차 엔트로피를 지나 [1.,0.,0.]으로 변화하게 되면 우리가 원하는 원핫인코딩 값, \n",
    "#즉 하나만 1이고 나머지는 모두 0인 형태로 전환시킬 수 있음\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X,Y_encoded,epochs=50,batch_size=1)\n",
    "\n",
    "print(\"\\n Accuracy: %.4f\" % model.evaluate(X,Y_encoded)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       208 non-null    float64\n",
      " 1   1       208 non-null    float64\n",
      " 2   2       208 non-null    float64\n",
      " 3   3       208 non-null    float64\n",
      " 4   4       208 non-null    float64\n",
      " 5   5       208 non-null    float64\n",
      " 6   6       208 non-null    float64\n",
      " 7   7       208 non-null    float64\n",
      " 8   8       208 non-null    float64\n",
      " 9   9       208 non-null    float64\n",
      " 10  10      208 non-null    float64\n",
      " 11  11      208 non-null    float64\n",
      " 12  12      208 non-null    float64\n",
      " 13  13      208 non-null    float64\n",
      " 14  14      208 non-null    float64\n",
      " 15  15      208 non-null    float64\n",
      " 16  16      208 non-null    float64\n",
      " 17  17      208 non-null    float64\n",
      " 18  18      208 non-null    float64\n",
      " 19  19      208 non-null    float64\n",
      " 20  20      208 non-null    float64\n",
      " 21  21      208 non-null    float64\n",
      " 22  22      208 non-null    float64\n",
      " 23  23      208 non-null    float64\n",
      " 24  24      208 non-null    float64\n",
      " 25  25      208 non-null    float64\n",
      " 26  26      208 non-null    float64\n",
      " 27  27      208 non-null    float64\n",
      " 28  28      208 non-null    float64\n",
      " 29  29      208 non-null    float64\n",
      " 30  30      208 non-null    float64\n",
      " 31  31      208 non-null    float64\n",
      " 32  32      208 non-null    float64\n",
      " 33  33      208 non-null    float64\n",
      " 34  34      208 non-null    float64\n",
      " 35  35      208 non-null    float64\n",
      " 36  36      208 non-null    float64\n",
      " 37  37      208 non-null    float64\n",
      " 38  38      208 non-null    float64\n",
      " 39  39      208 non-null    float64\n",
      " 40  40      208 non-null    float64\n",
      " 41  41      208 non-null    float64\n",
      " 42  42      208 non-null    float64\n",
      " 43  43      208 non-null    float64\n",
      " 44  44      208 non-null    float64\n",
      " 45  45      208 non-null    float64\n",
      " 46  46      208 non-null    float64\n",
      " 47  47      208 non-null    float64\n",
      " 48  48      208 non-null    float64\n",
      " 49  49      208 non-null    float64\n",
      " 50  50      208 non-null    float64\n",
      " 51  51      208 non-null    float64\n",
      " 52  52      208 non-null    float64\n",
      " 53  53      208 non-null    float64\n",
      " 54  54      208 non-null    float64\n",
      " 55  55      208 non-null    float64\n",
      " 56  56      208 non-null    float64\n",
      " 57  57      208 non-null    float64\n",
      " 58  58      208 non-null    float64\n",
      " 59  59      208 non-null    float64\n",
      " 60  60      208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 99.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sonar.csv', header=None)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed값 설정\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "42/42 [==============================] - 0s 673us/step - loss: 0.2468 - accuracy: 0.5096\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 0s 635us/step - loss: 0.2269 - accuracy: 0.6971\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 0s 675us/step - loss: 0.2153 - accuracy: 0.7115\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 0s 685us/step - loss: 0.2048 - accuracy: 0.7115\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 0s 723us/step - loss: 0.1969 - accuracy: 0.7260\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 0s 678us/step - loss: 0.1871 - accuracy: 0.7692\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 0s 676us/step - loss: 0.1768 - accuracy: 0.7788\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 0s 672us/step - loss: 0.1715 - accuracy: 0.7837\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.1620 - accuracy: 0.8125\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 0s 662us/step - loss: 0.1619 - accuracy: 0.7981\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 0s 671us/step - loss: 0.1514 - accuracy: 0.7933\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.1584 - accuracy: 0.7596\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 0s 709us/step - loss: 0.1447 - accuracy: 0.8173\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 0s 668us/step - loss: 0.1451 - accuracy: 0.8029\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 0s 634us/step - loss: 0.1411 - accuracy: 0.8125\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 0s 707us/step - loss: 0.1364 - accuracy: 0.8173\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 0s 654us/step - loss: 0.1360 - accuracy: 0.8077\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 0s 656us/step - loss: 0.1322 - accuracy: 0.8317\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 0s 686us/step - loss: 0.1320 - accuracy: 0.8173\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 0s 686us/step - loss: 0.1289 - accuracy: 0.8413\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 0s 677us/step - loss: 0.1270 - accuracy: 0.8317\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 0s 643us/step - loss: 0.1229 - accuracy: 0.8413\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 0s 697us/step - loss: 0.1253 - accuracy: 0.8269\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 0s 730us/step - loss: 0.1192 - accuracy: 0.8462\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 0s 736us/step - loss: 0.1212 - accuracy: 0.8221\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 0s 703us/step - loss: 0.1196 - accuracy: 0.8317\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 0s 672us/step - loss: 0.1159 - accuracy: 0.8413\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 0s 675us/step - loss: 0.1177 - accuracy: 0.8317\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 0s 689us/step - loss: 0.1135 - accuracy: 0.8558\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 0s 684us/step - loss: 0.1164 - accuracy: 0.8269\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 0s 724us/step - loss: 0.1165 - accuracy: 0.8269\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 0s 712us/step - loss: 0.1105 - accuracy: 0.8654\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 0s 704us/step - loss: 0.1132 - accuracy: 0.8317\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 0s 707us/step - loss: 0.1076 - accuracy: 0.8654\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 0s 665us/step - loss: 0.1061 - accuracy: 0.8606\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 0s 703us/step - loss: 0.1117 - accuracy: 0.8413\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 0s 706us/step - loss: 0.1066 - accuracy: 0.8510\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 0s 706us/step - loss: 0.1048 - accuracy: 0.8606\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.80 - 0s 691us/step - loss: 0.1059 - accuracy: 0.8558\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 0s 686us/step - loss: 0.1003 - accuracy: 0.8702\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 0s 725us/step - loss: 0.1026 - accuracy: 0.8654\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 0s 683us/step - loss: 0.0975 - accuracy: 0.8702\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 0s 699us/step - loss: 0.0994 - accuracy: 0.8894\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 0s 674us/step - loss: 0.0990 - accuracy: 0.8654\n",
      "Epoch 45/200\n",
      "42/42 [==============================] - 0s 692us/step - loss: 0.0962 - accuracy: 0.8846\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 0s 666us/step - loss: 0.0965 - accuracy: 0.8798\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.80 - 0s 682us/step - loss: 0.1025 - accuracy: 0.8558\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 0s 723us/step - loss: 0.0967 - accuracy: 0.8798\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 0s 700us/step - loss: 0.0940 - accuracy: 0.8750\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 0s 675us/step - loss: 0.0917 - accuracy: 0.8942\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 0s 654us/step - loss: 0.0896 - accuracy: 0.8894\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 0s 668us/step - loss: 0.0886 - accuracy: 0.8942\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 0s 688us/step - loss: 0.0900 - accuracy: 0.8846\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 0s 703us/step - loss: 0.0866 - accuracy: 0.8942\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 0s 703us/step - loss: 0.0846 - accuracy: 0.8990\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 0s 692us/step - loss: 0.0836 - accuracy: 0.9038\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 0s 669us/step - loss: 0.0843 - accuracy: 0.9038\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 0s 701us/step - loss: 0.0887 - accuracy: 0.8798\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 0s 716us/step - loss: 0.0794 - accuracy: 0.9327\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 0s 712us/step - loss: 0.0887 - accuracy: 0.8750\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 0s 729us/step - loss: 0.0808 - accuracy: 0.9135\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 0s 708us/step - loss: 0.0767 - accuracy: 0.9135\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 0s 694us/step - loss: 0.0748 - accuracy: 0.9231\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 0s 666us/step - loss: 0.0740 - accuracy: 0.9231\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 0s 638us/step - loss: 0.0751 - accuracy: 0.9135\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 0s 715us/step - loss: 0.0732 - accuracy: 0.9135\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 0s 735us/step - loss: 0.0739 - accuracy: 0.9135\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 0s 701us/step - loss: 0.0721 - accuracy: 0.9279\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 0s 668us/step - loss: 0.0735 - accuracy: 0.9135\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 0s 670us/step - loss: 0.0773 - accuracy: 0.8990\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 0s 683us/step - loss: 0.0747 - accuracy: 0.8990\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 0s 703us/step - loss: 0.0766 - accuracy: 0.8990\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 0s 684us/step - loss: 0.0709 - accuracy: 0.9279\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 0s 704us/step - loss: 0.0713 - accuracy: 0.9135\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 1.00 - 0s 670us/step - loss: 0.0689 - accuracy: 0.9135\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 0s 667us/step - loss: 0.0748 - accuracy: 0.9038\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 0s 712us/step - loss: 0.0649 - accuracy: 0.9279\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 0s 708us/step - loss: 0.0636 - accuracy: 0.9231\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 0s 688us/step - loss: 0.0668 - accuracy: 0.9279\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 658us/step - loss: 0.0615 - accuracy: 0.9327\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 0s 708us/step - loss: 0.0599 - accuracy: 0.9423\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 0s 703us/step - loss: 0.0599 - accuracy: 0.9423\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 0s 703us/step - loss: 0.0634 - accuracy: 0.9375\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 0s 699us/step - loss: 0.0574 - accuracy: 0.9471\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 0s 689us/step - loss: 0.0563 - accuracy: 0.9423\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 0s 676us/step - loss: 0.0569 - accuracy: 0.9327\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 0s 665us/step - loss: 0.0542 - accuracy: 0.9471\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 0s 654us/step - loss: 0.0566 - accuracy: 0.9375\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 0s 680us/step - loss: 0.0535 - accuracy: 0.9423\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 0s 716us/step - loss: 0.0526 - accuracy: 0.9519\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.0524 - accuracy: 0.9327\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 0s 715us/step - loss: 0.0541 - accuracy: 0.9519\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 0s 644us/step - loss: 0.0529 - accuracy: 0.9519\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 0s 696us/step - loss: 0.0508 - accuracy: 0.9519\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 0s 736us/step - loss: 0.0521 - accuracy: 0.9423\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 0s 709us/step - loss: 0.0494 - accuracy: 0.9567\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 0s 704us/step - loss: 0.0499 - accuracy: 0.9375\n",
      "Epoch 98/200\n",
      "42/42 [==============================] - 0s 760us/step - loss: 0.0467 - accuracy: 0.9712\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 0s 647us/step - loss: 0.0468 - accuracy: 0.9567\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 0s 714us/step - loss: 0.0465 - accuracy: 0.9519\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 0s 720us/step - loss: 0.0449 - accuracy: 0.9615\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 0s 704us/step - loss: 0.0429 - accuracy: 0.9712\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 0s 702us/step - loss: 0.0470 - accuracy: 0.9519\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 0s 717us/step - loss: 0.0434 - accuracy: 0.9615\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 0s 694us/step - loss: 0.0477 - accuracy: 0.9519\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 0s 663us/step - loss: 0.0415 - accuracy: 0.9615\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 0s 713us/step - loss: 0.0438 - accuracy: 0.9712\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 0s 709us/step - loss: 0.0413 - accuracy: 0.9615\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 0s 685us/step - loss: 0.0415 - accuracy: 0.9663\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 0s 710us/step - loss: 0.0396 - accuracy: 0.9712\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 0s 690us/step - loss: 0.0413 - accuracy: 0.9712\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 0s 679us/step - loss: 0.0385 - accuracy: 0.9615\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 0s 685us/step - loss: 0.0369 - accuracy: 0.9712\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 0s 718us/step - loss: 0.0386 - accuracy: 0.9712\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 0s 677us/step - loss: 0.0361 - accuracy: 0.9808\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 0s 687us/step - loss: 0.0358 - accuracy: 0.9712\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 0s 669us/step - loss: 0.0360 - accuracy: 0.9760\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 0s 708us/step - loss: 0.0373 - accuracy: 0.9712\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 0s 706us/step - loss: 0.0349 - accuracy: 0.9808\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 0s 684us/step - loss: 0.0331 - accuracy: 0.9663\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 0s 740us/step - loss: 0.0329 - accuracy: 0.9760\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 0s 715us/step - loss: 0.0316 - accuracy: 0.9663\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 0s 687us/step - loss: 0.0331 - accuracy: 0.9760\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 0s 715us/step - loss: 0.0320 - accuracy: 0.9760\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 0s 685us/step - loss: 0.0317 - accuracy: 0.9712\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 0s 693us/step - loss: 0.0336 - accuracy: 0.9808\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 0s 663us/step - loss: 0.0326 - accuracy: 0.9663\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 0s 748us/step - loss: 0.0299 - accuracy: 0.9808\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 0s 709us/step - loss: 0.0375 - accuracy: 0.9663\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 0s 716us/step - loss: 0.0343 - accuracy: 0.9567\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 0s 690us/step - loss: 0.0290 - accuracy: 0.9808\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 0s 663us/step - loss: 0.0276 - accuracy: 0.9808\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 0s 690us/step - loss: 0.0291 - accuracy: 0.9808\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 0s 676us/step - loss: 0.0276 - accuracy: 0.9808\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 0s 682us/step - loss: 0.0271 - accuracy: 0.9808\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 0s 662us/step - loss: 0.0275 - accuracy: 0.9808\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 0s 689us/step - loss: 0.0260 - accuracy: 0.9856\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 0s 585us/step - loss: 0.0256 - accuracy: 0.9808\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 0s 607us/step - loss: 0.0243 - accuracy: 0.9808\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 0s 706us/step - loss: 0.0259 - accuracy: 0.9856\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 0s 690us/step - loss: 0.0254 - accuracy: 0.9808\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 0s 662us/step - loss: 0.0234 - accuracy: 0.9856\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 0s 655us/step - loss: 0.0246 - accuracy: 0.9856\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 0s 686us/step - loss: 0.0241 - accuracy: 0.9856\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 0s 652us/step - loss: 0.0243 - accuracy: 0.9856\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 0s 667us/step - loss: 0.0221 - accuracy: 0.9856\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 0s 691us/step - loss: 0.0224 - accuracy: 0.9856\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 0s 690us/step - loss: 0.0232 - accuracy: 0.9856\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 0s 670us/step - loss: 0.0221 - accuracy: 0.9808\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 0s 699us/step - loss: 0.0202 - accuracy: 0.9856\n",
      "Epoch 151/200\n",
      "42/42 [==============================] - 0s 649us/step - loss: 0.0224 - accuracy: 0.9856\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 0s 670us/step - loss: 0.0202 - accuracy: 0.9856\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 0s 714us/step - loss: 0.0200 - accuracy: 0.9856\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 0s 671us/step - loss: 0.0199 - accuracy: 0.9808\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 0s 694us/step - loss: 0.0195 - accuracy: 0.9808\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 0s 691us/step - loss: 0.0183 - accuracy: 0.9856\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 0s 696us/step - loss: 0.0205 - accuracy: 0.9856\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 0s 675us/step - loss: 0.0177 - accuracy: 0.9856\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 660us/step - loss: 0.0165 - accuracy: 0.9856\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 0s 696us/step - loss: 0.0168 - accuracy: 0.9856\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 0s 710us/step - loss: 0.0158 - accuracy: 0.9856\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 0s 685us/step - loss: 0.0146 - accuracy: 0.9856\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 0s 720us/step - loss: 0.0148 - accuracy: 0.9856\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 0s 690us/step - loss: 0.0157 - accuracy: 0.9856\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 0s 711us/step - loss: 0.0145 - accuracy: 0.9856\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 0s 640us/step - loss: 0.0144 - accuracy: 0.9856\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 0s 697us/step - loss: 0.0128 - accuracy: 0.9904\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 0s 721us/step - loss: 0.0142 - accuracy: 0.9952\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 0s 706us/step - loss: 0.0134 - accuracy: 0.9856\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 0s 683us/step - loss: 0.0131 - accuracy: 0.9856\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 0s 702us/step - loss: 0.0118 - accuracy: 0.9904\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 0s 693us/step - loss: 0.0121 - accuracy: 0.9904\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 0s 691us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 0s 694us/step - loss: 0.0110 - accuracy: 0.9952\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 0s 652us/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 0s 668us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 0s 690us/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 0s 719us/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 0s 711us/step - loss: 0.0098 - accuracy: 0.9952\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 0s 668us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 0s 676us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 0s 664us/step - loss: 0.0080 - accuracy: 0.9952\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 0s 691us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 0s 692us/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 0s 665us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 0s 689us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 0s 679us/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 0s 688us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 0s 667us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 0s 676us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 0s 658us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 0s 671us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 0s 678us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 0s 634us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 0s 682us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 0s 674us/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 0s 636us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.00 - 0s 631us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 0s 682us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 0s 701us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "7/7 [==============================] - 0s 570us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "\n",
      " Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#문자열을 불러올때는 원핫인코딩을 사용해야함\n",
    "\n",
    "#데이터분류\n",
    "dataset=df.values\n",
    "X=dataset[:,0:60].astype(float)\n",
    "Y_obj=dataset[:,60]\n",
    "\n",
    "#원핫인코딩\n",
    "#문자열을 숫자로 변환\n",
    "e=LabelEncoder()\n",
    "e.fit(Y_obj)\n",
    "Y=e.transform(Y_obj)\n",
    "\n",
    "#모델 설정\n",
    "#최종 출력 값이 3개 중 하나여야 하므로 출력층에 해당하는 덴스를 3개로 만들어줌\n",
    "model=Sequential()\n",
    "model.add(Dense(24,input_dim=60,activation='relu'))\n",
    "model.add(Dense(10,activation='relu'))#활성화함수\n",
    "#소프트맥스: 총합이 1인 형태로 바꿔서 계산해 주는 함수\n",
    "#합계가 1인 형태로 변환하면 큰 값이 두드러지게 나타나고 작은 값은 더 작아짐\n",
    "#이 값이 교차 엔트로피를 지나 [1.,0.,0.]으로 변화하게 되면 우리가 원하는 원핫인코딩 값, \n",
    "#즉 하나만 1이고 나머지는 모두 0인 형태로 전환시킬 수 있음\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X,Y,epochs=200,batch_size=5)\n",
    "\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X,Y)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과적합 이해하기\n",
    "-----\n",
    "- 학습을 진행해도 테스트 결과가 더 이상 좋아지지 않는 지점에서 학습을 멈춰야 함\n",
    "- 은닉층이 늘어날수록 학습셋의 예측률이 점점 올라가다가 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "25/25 [==============================] - 0s 680us/step - loss: 0.2556 - accuracy: 0.4758\n",
      "Epoch 2/130\n",
      "25/25 [==============================] - 0s 720us/step - loss: 0.2443 - accuracy: 0.5968\n",
      "Epoch 3/130\n",
      "25/25 [==============================] - 0s 650us/step - loss: 0.2369 - accuracy: 0.6290\n",
      "Epoch 4/130\n",
      "25/25 [==============================] - 0s 644us/step - loss: 0.2338 - accuracy: 0.6613\n",
      "Epoch 5/130\n",
      "25/25 [==============================] - 0s 714us/step - loss: 0.2270 - accuracy: 0.7097\n",
      "Epoch 6/130\n",
      "25/25 [==============================] - 0s 680us/step - loss: 0.2230 - accuracy: 0.7177\n",
      "Epoch 7/130\n",
      "25/25 [==============================] - 0s 741us/step - loss: 0.2160 - accuracy: 0.7339\n",
      "Epoch 8/130\n",
      "25/25 [==============================] - 0s 720us/step - loss: 0.2097 - accuracy: 0.7258\n",
      "Epoch 9/130\n",
      "25/25 [==============================] - 0s 725us/step - loss: 0.2058 - accuracy: 0.7339\n",
      "Epoch 10/130\n",
      "25/25 [==============================] - 0s 711us/step - loss: 0.1999 - accuracy: 0.7177\n",
      "Epoch 11/130\n",
      "25/25 [==============================] - 0s 709us/step - loss: 0.1933 - accuracy: 0.7500\n",
      "Epoch 12/130\n",
      "25/25 [==============================] - 0s 722us/step - loss: 0.1886 - accuracy: 0.7419\n",
      "Epoch 13/130\n",
      "25/25 [==============================] - 0s 763us/step - loss: 0.1822 - accuracy: 0.7500\n",
      "Epoch 14/130\n",
      "25/25 [==============================] - 0s 703us/step - loss: 0.1810 - accuracy: 0.7419\n",
      "Epoch 15/130\n",
      "25/25 [==============================] - 0s 679us/step - loss: 0.1751 - accuracy: 0.7661\n",
      "Epoch 16/130\n",
      "25/25 [==============================] - 0s 709us/step - loss: 0.1702 - accuracy: 0.7500\n",
      "Epoch 17/130\n",
      "25/25 [==============================] - 0s 714us/step - loss: 0.1643 - accuracy: 0.7742\n",
      "Epoch 18/130\n",
      "25/25 [==============================] - 0s 724us/step - loss: 0.1597 - accuracy: 0.7903\n",
      "Epoch 19/130\n",
      "25/25 [==============================] - 0s 714us/step - loss: 0.1551 - accuracy: 0.7823\n",
      "Epoch 20/130\n",
      "25/25 [==============================] - 0s 678us/step - loss: 0.1515 - accuracy: 0.8145\n",
      "Epoch 21/130\n",
      "25/25 [==============================] - 0s 724us/step - loss: 0.1518 - accuracy: 0.8145\n",
      "Epoch 22/130\n",
      "25/25 [==============================] - 0s 718us/step - loss: 0.1487 - accuracy: 0.7742\n",
      "Epoch 23/130\n",
      "25/25 [==============================] - 0s 730us/step - loss: 0.1483 - accuracy: 0.8065\n",
      "Epoch 24/130\n",
      "25/25 [==============================] - 0s 743us/step - loss: 0.1432 - accuracy: 0.8145\n",
      "Epoch 25/130\n",
      "25/25 [==============================] - 0s 722us/step - loss: 0.1373 - accuracy: 0.8710\n",
      "Epoch 26/130\n",
      "25/25 [==============================] - 0s 711us/step - loss: 0.1347 - accuracy: 0.8065\n",
      "Epoch 27/130\n",
      "25/25 [==============================] - 0s 678us/step - loss: 0.1331 - accuracy: 0.8387\n",
      "Epoch 28/130\n",
      "25/25 [==============================] - 0s 681us/step - loss: 0.1301 - accuracy: 0.8387\n",
      "Epoch 29/130\n",
      "25/25 [==============================] - 0s 739us/step - loss: 0.1273 - accuracy: 0.8387\n",
      "Epoch 30/130\n",
      "25/25 [==============================] - 0s 709us/step - loss: 0.1257 - accuracy: 0.8629\n",
      "Epoch 31/130\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.1237 - accuracy: 0.8387\n",
      "Epoch 32/130\n",
      "25/25 [==============================] - 0s 789us/step - loss: 0.1191 - accuracy: 0.8548\n",
      "Epoch 33/130\n",
      "25/25 [==============================] - 0s 681us/step - loss: 0.1166 - accuracy: 0.8710\n",
      "Epoch 34/130\n",
      "25/25 [==============================] - 0s 700us/step - loss: 0.1149 - accuracy: 0.8871\n",
      "Epoch 35/130\n",
      "25/25 [==============================] - 0s 715us/step - loss: 0.1168 - accuracy: 0.8629\n",
      "Epoch 36/130\n",
      "25/25 [==============================] - 0s 749us/step - loss: 0.1144 - accuracy: 0.8387\n",
      "Epoch 37/130\n",
      "25/25 [==============================] - 0s 678us/step - loss: 0.1137 - accuracy: 0.8468\n",
      "Epoch 38/130\n",
      "25/25 [==============================] - 0s 745us/step - loss: 0.1080 - accuracy: 0.8790\n",
      "Epoch 39/130\n",
      "25/25 [==============================] - 0s 782us/step - loss: 0.1075 - accuracy: 0.8871\n",
      "Epoch 40/130\n",
      "25/25 [==============================] - 0s 698us/step - loss: 0.1077 - accuracy: 0.8952\n",
      "Epoch 41/130\n",
      "25/25 [==============================] - 0s 693us/step - loss: 0.1026 - accuracy: 0.8710\n",
      "Epoch 42/130\n",
      "25/25 [==============================] - 0s 681us/step - loss: 0.1008 - accuracy: 0.8952\n",
      "Epoch 43/130\n",
      "25/25 [==============================] - 0s 684us/step - loss: 0.1042 - accuracy: 0.8790\n",
      "Epoch 44/130\n",
      "25/25 [==============================] - 0s 710us/step - loss: 0.1049 - accuracy: 0.8548\n",
      "Epoch 45/130\n",
      "25/25 [==============================] - 0s 711us/step - loss: 0.0955 - accuracy: 0.8790\n",
      "Epoch 46/130\n",
      "25/25 [==============================] - 0s 682us/step - loss: 0.0945 - accuracy: 0.8790\n",
      "Epoch 47/130\n",
      "25/25 [==============================] - 0s 711us/step - loss: 0.0946 - accuracy: 0.9113\n",
      "Epoch 48/130\n",
      "25/25 [==============================] - 0s 676us/step - loss: 0.0916 - accuracy: 0.9032\n",
      "Epoch 49/130\n",
      "25/25 [==============================] - 0s 712us/step - loss: 0.0943 - accuracy: 0.8871\n",
      "Epoch 50/130\n",
      "25/25 [==============================] - 0s 716us/step - loss: 0.0942 - accuracy: 0.8952\n",
      "Epoch 51/130\n",
      "25/25 [==============================] - 0s 649us/step - loss: 0.0867 - accuracy: 0.9032\n",
      "Epoch 52/130\n",
      "25/25 [==============================] - 0s 714us/step - loss: 0.0880 - accuracy: 0.9113\n",
      "Epoch 53/130\n",
      "25/25 [==============================] - 0s 756us/step - loss: 0.0890 - accuracy: 0.8952\n",
      "Epoch 54/130\n",
      "25/25 [==============================] - 0s 710us/step - loss: 0.0897 - accuracy: 0.9032\n",
      "Epoch 55/130\n",
      "25/25 [==============================] - 0s 676us/step - loss: 0.0830 - accuracy: 0.9194\n",
      "Epoch 56/130\n",
      "25/25 [==============================] - 0s 709us/step - loss: 0.0954 - accuracy: 0.8871\n",
      "Epoch 57/130\n",
      "25/25 [==============================] - 0s 669us/step - loss: 0.0849 - accuracy: 0.8790\n",
      "Epoch 58/130\n",
      "25/25 [==============================] - 0s 714us/step - loss: 0.0871 - accuracy: 0.8871\n",
      "Epoch 59/130\n",
      "25/25 [==============================] - 0s 688us/step - loss: 0.0820 - accuracy: 0.8952\n",
      "Epoch 60/130\n",
      "25/25 [==============================] - 0s 690us/step - loss: 0.0775 - accuracy: 0.9113\n",
      "Epoch 61/130\n",
      "25/25 [==============================] - 0s 700us/step - loss: 0.0757 - accuracy: 0.9113\n",
      "Epoch 62/130\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.0766 - accuracy: 0.9032\n",
      "Epoch 63/130\n",
      "25/25 [==============================] - 0s 676us/step - loss: 0.0776 - accuracy: 0.9032\n",
      "Epoch 64/130\n",
      "25/25 [==============================] - 0s 675us/step - loss: 0.0739 - accuracy: 0.9113\n",
      "Epoch 65/130\n",
      "25/25 [==============================] - 0s 686us/step - loss: 0.0689 - accuracy: 0.9355\n",
      "Epoch 66/130\n",
      "25/25 [==============================] - 0s 705us/step - loss: 0.0740 - accuracy: 0.9274\n",
      "Epoch 67/130\n",
      "25/25 [==============================] - 0s 727us/step - loss: 0.0677 - accuracy: 0.9274\n",
      "Epoch 68/130\n",
      "25/25 [==============================] - 0s 718us/step - loss: 0.0664 - accuracy: 0.9274\n",
      "Epoch 69/130\n",
      "25/25 [==============================] - 0s 700us/step - loss: 0.0706 - accuracy: 0.9113\n",
      "Epoch 70/130\n",
      "25/25 [==============================] - 0s 724us/step - loss: 0.0705 - accuracy: 0.9194\n",
      "Epoch 71/130\n",
      "25/25 [==============================] - 0s 763us/step - loss: 0.0654 - accuracy: 0.9274\n",
      "Epoch 72/130\n",
      "25/25 [==============================] - 0s 711us/step - loss: 0.0668 - accuracy: 0.9194\n",
      "Epoch 73/130\n",
      "25/25 [==============================] - 0s 679us/step - loss: 0.0637 - accuracy: 0.9355\n",
      "Epoch 74/130\n",
      "25/25 [==============================] - 0s 710us/step - loss: 0.0619 - accuracy: 0.9355\n",
      "Epoch 75/130\n",
      "25/25 [==============================] - 0s 679us/step - loss: 0.0612 - accuracy: 0.9355\n",
      "Epoch 76/130\n",
      "25/25 [==============================] - 0s 728us/step - loss: 0.0586 - accuracy: 0.9355\n",
      "Epoch 77/130\n",
      "25/25 [==============================] - 0s 678us/step - loss: 0.0624 - accuracy: 0.9435\n",
      "Epoch 78/130\n",
      "25/25 [==============================] - 0s 723us/step - loss: 0.0574 - accuracy: 0.9435\n",
      "Epoch 79/130\n",
      "25/25 [==============================] - 0s 703us/step - loss: 0.0667 - accuracy: 0.9274\n",
      "Epoch 80/130\n",
      "25/25 [==============================] - 0s 711us/step - loss: 0.0634 - accuracy: 0.9355\n",
      "Epoch 81/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 718us/step - loss: 0.0559 - accuracy: 0.9516\n",
      "Epoch 82/130\n",
      "25/25 [==============================] - 0s 758us/step - loss: 0.0591 - accuracy: 0.9355\n",
      "Epoch 83/130\n",
      "25/25 [==============================] - 0s 777us/step - loss: 0.0575 - accuracy: 0.9355\n",
      "Epoch 84/130\n",
      "25/25 [==============================] - 0s 710us/step - loss: 0.0538 - accuracy: 0.9274\n",
      "Epoch 85/130\n",
      "25/25 [==============================] - 0s 677us/step - loss: 0.0532 - accuracy: 0.9516\n",
      "Epoch 86/130\n",
      "25/25 [==============================] - 0s 681us/step - loss: 0.0517 - accuracy: 0.9516\n",
      "Epoch 87/130\n",
      "25/25 [==============================] - 0s 636us/step - loss: 0.0503 - accuracy: 0.9597\n",
      "Epoch 88/130\n",
      "25/25 [==============================] - 0s 719us/step - loss: 0.0502 - accuracy: 0.9597\n",
      "Epoch 89/130\n",
      "25/25 [==============================] - 0s 731us/step - loss: 0.0463 - accuracy: 0.9597\n",
      "Epoch 90/130\n",
      "25/25 [==============================] - 0s 678us/step - loss: 0.0459 - accuracy: 0.9677\n",
      "Epoch 91/130\n",
      "25/25 [==============================] - 0s 741us/step - loss: 0.0457 - accuracy: 0.9516\n",
      "Epoch 92/130\n",
      "25/25 [==============================] - 0s 721us/step - loss: 0.0469 - accuracy: 0.9516\n",
      "Epoch 93/130\n",
      "25/25 [==============================] - 0s 745us/step - loss: 0.0454 - accuracy: 0.9597\n",
      "Epoch 94/130\n",
      "25/25 [==============================] - 0s 737us/step - loss: 0.0560 - accuracy: 0.9355\n",
      "Epoch 95/130\n",
      "25/25 [==============================] - 0s 721us/step - loss: 0.0436 - accuracy: 0.9516\n",
      "Epoch 96/130\n",
      "25/25 [==============================] - 0s 677us/step - loss: 0.0401 - accuracy: 0.9839\n",
      "Epoch 97/130\n",
      "25/25 [==============================] - 0s 696us/step - loss: 0.0424 - accuracy: 0.9919\n",
      "Epoch 98/130\n",
      "25/25 [==============================] - 0s 747us/step - loss: 0.0386 - accuracy: 0.9839\n",
      "Epoch 99/130\n",
      "25/25 [==============================] - 0s 696us/step - loss: 0.0397 - accuracy: 0.9758\n",
      "Epoch 100/130\n",
      "25/25 [==============================] - 0s 651us/step - loss: 0.0373 - accuracy: 0.9839\n",
      "Epoch 101/130\n",
      "25/25 [==============================] - 0s 715us/step - loss: 0.0348 - accuracy: 0.9758\n",
      "Epoch 102/130\n",
      "25/25 [==============================] - 0s 671us/step - loss: 0.0508 - accuracy: 0.9516\n",
      "Epoch 103/130\n",
      "25/25 [==============================] - 0s 666us/step - loss: 0.0376 - accuracy: 0.9758\n",
      "Epoch 104/130\n",
      "25/25 [==============================] - 0s 705us/step - loss: 0.0363 - accuracy: 0.9758\n",
      "Epoch 105/130\n",
      "25/25 [==============================] - 0s 751us/step - loss: 0.0359 - accuracy: 0.9758\n",
      "Epoch 106/130\n",
      "25/25 [==============================] - 0s 719us/step - loss: 0.0343 - accuracy: 0.9758\n",
      "Epoch 107/130\n",
      "25/25 [==============================] - 0s 747us/step - loss: 0.0333 - accuracy: 0.9758\n",
      "Epoch 108/130\n",
      "25/25 [==============================] - 0s 717us/step - loss: 0.0343 - accuracy: 0.9839\n",
      "Epoch 109/130\n",
      "25/25 [==============================] - 0s 681us/step - loss: 0.0310 - accuracy: 0.9919\n",
      "Epoch 110/130\n",
      "25/25 [==============================] - 0s 698us/step - loss: 0.0314 - accuracy: 0.9919\n",
      "Epoch 111/130\n",
      "25/25 [==============================] - 0s 690us/step - loss: 0.0302 - accuracy: 0.9839\n",
      "Epoch 112/130\n",
      "25/25 [==============================] - 0s 713us/step - loss: 0.0320 - accuracy: 0.9839\n",
      "Epoch 113/130\n",
      "25/25 [==============================] - 0s 721us/step - loss: 0.0285 - accuracy: 0.9839\n",
      "Epoch 114/130\n",
      "25/25 [==============================] - 0s 733us/step - loss: 0.0300 - accuracy: 0.9758\n",
      "Epoch 115/130\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.0269 - accuracy: 0.9919\n",
      "Epoch 116/130\n",
      "25/25 [==============================] - 0s 713us/step - loss: 0.0273 - accuracy: 0.9839\n",
      "Epoch 117/130\n",
      "25/25 [==============================] - 0s 739us/step - loss: 0.0279 - accuracy: 0.9919\n",
      "Epoch 118/130\n",
      "25/25 [==============================] - 0s 714us/step - loss: 0.0359 - accuracy: 0.9516\n",
      "Epoch 119/130\n",
      "25/25 [==============================] - 0s 741us/step - loss: 0.0307 - accuracy: 0.9839\n",
      "Epoch 120/130\n",
      "25/25 [==============================] - 0s 695us/step - loss: 0.0261 - accuracy: 0.9919\n",
      "Epoch 121/130\n",
      "25/25 [==============================] - 0s 699us/step - loss: 0.0240 - accuracy: 0.9919\n",
      "Epoch 122/130\n",
      "25/25 [==============================] - 0s 679us/step - loss: 0.0229 - accuracy: 0.9919\n",
      "Epoch 123/130\n",
      "25/25 [==============================] - 0s 727us/step - loss: 0.0277 - accuracy: 0.9919\n",
      "Epoch 124/130\n",
      "25/25 [==============================] - 0s 705us/step - loss: 0.0255 - accuracy: 0.9839\n",
      "Epoch 125/130\n",
      "25/25 [==============================] - 0s 655us/step - loss: 0.0245 - accuracy: 0.9919\n",
      "Epoch 126/130\n",
      "25/25 [==============================] - 0s 680us/step - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 127/130\n",
      "25/25 [==============================] - 0s 707us/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 128/130\n",
      "25/25 [==============================] - 0s 727us/step - loss: 0.0238 - accuracy: 0.9919\n",
      "Epoch 129/130\n",
      "25/25 [==============================] - 0s 716us/step - loss: 0.0190 - accuracy: 0.9919\n",
      "Epoch 130/130\n",
      "25/25 [==============================] - 0s 711us/step - loss: 0.0232 - accuracy: 0.9839\n",
      "3/3 [==============================] - 0s 437us/step - loss: 0.1627 - accuracy: 0.7500\n",
      "\n",
      " Test Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "#데이터분류\n",
    "dataset=df.values\n",
    "X=dataset[:,0:60].astype(float)\n",
    "Y_obj=dataset[:,60]\n",
    "\n",
    "#원핫인코딩\n",
    "#문자열을 숫자로 변환\n",
    "e=LabelEncoder()\n",
    "e.fit(Y_obj)\n",
    "Y=e.transform(Y_obj)\n",
    "\n",
    "#모델 설정\n",
    "#최종 출력 값이 3개 중 하나여야 하므로 출력층에 해당하는 덴스를 3개로 만들어줌\n",
    "model=Sequential()\n",
    "model.add(Dense(24,input_dim=60,activation='relu'))\n",
    "model.add(Dense(10,activation='relu'))#활성화함수\n",
    "#소프트맥스: 총합이 1인 형태로 바꿔서 계산해 주는 함수\n",
    "#합계가 1인 형태로 변환하면 큰 값이 두드러지게 나타나고 작은 값은 더 작아짐\n",
    "#이 값이 교차 엔트로피를 지나 [1.,0.,0.]으로 변화하게 되면 우리가 원하는 원핫인코딩 값, \n",
    "#즉 하나만 1이고 나머지는 모두 0인 형태로 전환시킬 수 있음\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 학습셋과 테스트셋의 구분\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, \n",
    "                                                    random_state=seed)\n",
    "model.fit(X_train, Y_train, epochs=130, batch_size=5)\n",
    "\n",
    "# 테스트셋에 모델 적용\n",
    "print(\"\\n Test Accuracy: %.4f\"% (model.evaluate(X_test, Y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#모델의 결과 저장\n",
    "model.save('my_model.h5')\n",
    "\n",
    "#불러올때 실행코드\n",
    "model=load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "38/38 [==============================] - 0s 657us/step - loss: 0.0237 - accuracy: 0.9947\n",
      "Epoch 2/150\n",
      "38/38 [==============================] - 0s 658us/step - loss: 0.0249 - accuracy: 0.9787\n",
      "Epoch 3/150\n",
      "38/38 [==============================] - 0s 694us/step - loss: 0.0283 - accuracy: 0.9681\n",
      "Epoch 4/150\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.0201 - accuracy: 0.9947\n",
      "Epoch 5/150\n",
      "38/38 [==============================] - 0s 684us/step - loss: 0.0180 - accuracy: 0.9947\n",
      "Epoch 6/150\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.0229 - accuracy: 0.9840\n",
      "Epoch 7/150\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.0166 - accuracy: 0.9947\n",
      "Epoch 8/150\n",
      "38/38 [==============================] - 0s 694us/step - loss: 0.0158 - accuracy: 0.9947\n",
      "Epoch 9/150\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.0159 - accuracy: 0.9947\n",
      "Epoch 10/150\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.0161 - accuracy: 0.9947\n",
      "Epoch 11/150\n",
      "38/38 [==============================] - 0s 689us/step - loss: 0.0171 - accuracy: 0.9894\n",
      "Epoch 12/150\n",
      "38/38 [==============================] - 0s 676us/step - loss: 0.0138 - accuracy: 0.9947\n",
      "Epoch 13/150\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.0147 - accuracy: 0.9947\n",
      "Epoch 14/150\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.0130 - accuracy: 0.9947\n",
      "Epoch 15/150\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.0130 - accuracy: 0.9947\n",
      "Epoch 16/150\n",
      "38/38 [==============================] - 0s 689us/step - loss: 0.0129 - accuracy: 0.9894\n",
      "Epoch 17/150\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.0124 - accuracy: 0.9947\n",
      "Epoch 18/150\n",
      "38/38 [==============================] - 0s 691us/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 19/150\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 20/150\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.0113 - accuracy: 0.9947\n",
      "Epoch 21/150\n",
      "38/38 [==============================] - 0s 691us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 22/150\n",
      "38/38 [==============================] - 0s 685us/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 23/150\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 24/150\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 25/150\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 26/150\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 27/150\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 28/150\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 29/150\n",
      "38/38 [==============================] - 0s 677us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 30/150\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 31/150\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 32/150\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 33/150\n",
      "38/38 [==============================] - 0s 673us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 34/150\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 35/150\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 36/150\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 37/150\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 38/150\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "38/38 [==============================] - 0s 679us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 40/150\n",
      "38/38 [==============================] - 0s 683us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 41/150\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.0075 - accuracy: 0.9947\n",
      "Epoch 42/150\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "38/38 [==============================] - 0s 648us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "38/38 [==============================] - 0s 692us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 47/150\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 48/150\n",
      "38/38 [==============================] - 0s 688us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 49/150\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 50/150\n",
      "38/38 [==============================] - 0s 717us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 51/150\n",
      "38/38 [==============================] - 0s 683us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "38/38 [==============================] - 0s 659us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "38/38 [==============================] - 0s 688us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "38/38 [==============================] - 0s 680us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "38/38 [==============================] - 0s 675us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "38/38 [==============================] - 0s 683us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "38/38 [==============================] - 0s 671us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "38/38 [==============================] - 0s 685us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "38/38 [==============================] - 0s 659us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "38/38 [==============================] - 0s 648us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "38/38 [==============================] - 0s 684us/step - loss: 9.0174e-04 - accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "38/38 [==============================] - 0s 706us/step - loss: 9.9004e-04 - accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "38/38 [==============================] - 0s 681us/step - loss: 9.1087e-04 - accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "38/38 [==============================] - 0s 692us/step - loss: 9.5163e-04 - accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "38/38 [==============================] - 0s 698us/step - loss: 8.7454e-04 - accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "38/38 [==============================] - 0s 680us/step - loss: 9.2428e-04 - accuracy: 1.0000\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 696us/step - loss: 9.1356e-04 - accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "38/38 [==============================] - 0s 696us/step - loss: 7.6998e-04 - accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "38/38 [==============================] - 0s 697us/step - loss: 7.0884e-04 - accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "38/38 [==============================] - 0s 700us/step - loss: 7.5097e-04 - accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "38/38 [==============================] - 0s 667us/step - loss: 8.0238e-04 - accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "38/38 [==============================] - 0s 679us/step - loss: 7.8305e-04 - accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "38/38 [==============================] - 0s 658us/step - loss: 7.2693e-04 - accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "38/38 [==============================] - 0s 690us/step - loss: 6.4144e-04 - accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "38/38 [==============================] - 0s 689us/step - loss: 5.9382e-04 - accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "38/38 [==============================] - 0s 681us/step - loss: 5.9008e-04 - accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "38/38 [==============================] - 0s 710us/step - loss: 6.2303e-04 - accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "38/38 [==============================] - 0s 692us/step - loss: 5.9323e-04 - accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "38/38 [==============================] - 0s 730us/step - loss: 5.8816e-04 - accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "38/38 [==============================] - 0s 686us/step - loss: 5.8392e-04 - accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "38/38 [==============================] - 0s 680us/step - loss: 6.4812e-04 - accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "38/38 [==============================] - 0s 723us/step - loss: 5.7013e-04 - accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "38/38 [==============================] - 0s 705us/step - loss: 5.2466e-04 - accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "38/38 [==============================] - 0s 646us/step - loss: 5.2155e-04 - accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "38/38 [==============================] - 0s 680us/step - loss: 5.2236e-04 - accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "38/38 [==============================] - 0s 676us/step - loss: 7.2441e-04 - accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "38/38 [==============================] - 0s 752us/step - loss: 4.7880e-04 - accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "38/38 [==============================] - 0s 704us/step - loss: 3.9309e-04 - accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "38/38 [==============================] - 0s 704us/step - loss: 4.1030e-04 - accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "38/38 [==============================] - 0s 674us/step - loss: 3.5999e-04 - accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "38/38 [==============================] - 0s 641us/step - loss: 3.4502e-04 - accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "38/38 [==============================] - 0s 682us/step - loss: 3.3893e-04 - accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "38/38 [==============================] - 0s 716us/step - loss: 3.1422e-04 - accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "38/38 [==============================] - 0s 693us/step - loss: 3.5073e-04 - accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "38/38 [==============================] - 0s 686us/step - loss: 3.1604e-04 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "38/38 [==============================] - 0s 684us/step - loss: 2.8883e-04 - accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "38/38 [==============================] - 0s 679us/step - loss: 3.1104e-04 - accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "38/38 [==============================] - 0s 712us/step - loss: 3.4260e-04 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "38/38 [==============================] - 0s 720us/step - loss: 2.8774e-04 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "38/38 [==============================] - 0s 646us/step - loss: 2.5986e-04 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "38/38 [==============================] - 0s 668us/step - loss: 2.9255e-04 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "38/38 [==============================] - 0s 679us/step - loss: 2.6935e-04 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "38/38 [==============================] - 0s 682us/step - loss: 2.5796e-04 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "38/38 [==============================] - 0s 656us/step - loss: 2.4460e-04 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "38/38 [==============================] - 0s 660us/step - loss: 2.5227e-04 - accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "38/38 [==============================] - 0s 661us/step - loss: 2.2837e-04 - accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "38/38 [==============================] - 0s 684us/step - loss: 2.3495e-04 - accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "38/38 [==============================] - 0s 662us/step - loss: 2.2255e-04 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "38/38 [==============================] - 0s 692us/step - loss: 2.1410e-04 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "38/38 [==============================] - 0s 675us/step - loss: 2.1985e-04 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "38/38 [==============================] - 0s 694us/step - loss: 2.0570e-04 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "38/38 [==============================] - 0s 667us/step - loss: 2.0994e-04 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "38/38 [==============================] - 0s 672us/step - loss: 1.9655e-04 - accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "38/38 [==============================] - 0s 680us/step - loss: 1.7860e-04 - accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "38/38 [==============================] - 0s 691us/step - loss: 1.8471e-04 - accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "38/38 [==============================] - 0s 660us/step - loss: 1.9352e-04 - accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "38/38 [==============================] - 0s 709us/step - loss: 2.2056e-04 - accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "38/38 [==============================] - 0s 697us/step - loss: 1.8402e-04 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "38/38 [==============================] - 0s 693us/step - loss: 1.6729e-04 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "38/38 [==============================] - 0s 678us/step - loss: 1.5595e-04 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "38/38 [==============================] - 0s 689us/step - loss: 1.6635e-04 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "38/38 [==============================] - 0s 674us/step - loss: 1.5021e-04 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "38/38 [==============================] - 0s 684us/step - loss: 1.6152e-04 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "38/38 [==============================] - 0s 723us/step - loss: 1.3614e-04 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "38/38 [==============================] - 0s 694us/step - loss: 1.5844e-04 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "38/38 [==============================] - 0s 709us/step - loss: 1.5545e-04 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "38/38 [==============================] - 0s 719us/step - loss: 1.3683e-04 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "38/38 [==============================] - 0s 655us/step - loss: 1.2919e-04 - accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "38/38 [==============================] - 0s 702us/step - loss: 1.2280e-04 - accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "38/38 [==============================] - 0s 669us/step - loss: 1.2834e-04 - accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "38/38 [==============================] - 0s 694us/step - loss: 1.1652e-04 - accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "38/38 [==============================] - 0s 671us/step - loss: 1.2606e-04 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "38/38 [==============================] - 0s 680us/step - loss: 1.1027e-04 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "38/38 [==============================] - 0s 685us/step - loss: 1.1333e-04 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "38/38 [==============================] - 0s 680us/step - loss: 1.0446e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000199FF897D38> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.8500\n",
      "\n",
      " Test Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=150, batch_size=5)\n",
    "model.save('my_model.h5')\n",
    "\n",
    "del model\n",
    "model=load_model('my_model.h5')\n",
    "\n",
    "print(\"\\n Test Accuracy: %.4f\"% (model.evaluate(X_test, Y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k겹 교차 검증\n",
    "앞서 가지고 있는 데이터의 70퍼센트를 학습셋으로 써야 했으므로 테스트셋은 30퍼신ㄷ트\n",
    "- 데이터셋을 여러개로 나눔\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.2426 - accuracy: 0.5455\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 621us/step - loss: 0.2260 - accuracy: 0.6578\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.2156 - accuracy: 0.7326\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2057 - accuracy: 0.7487\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.1946 - accuracy: 0.7219\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 680us/step - loss: 0.1843 - accuracy: 0.7540\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.1730 - accuracy: 0.7861\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 650us/step - loss: 0.1631 - accuracy: 0.8075\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 681us/step - loss: 0.1559 - accuracy: 0.8075\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 694us/step - loss: 0.1504 - accuracy: 0.8075\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.1457 - accuracy: 0.8128\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.1459 - accuracy: 0.7754\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 683us/step - loss: 0.1413 - accuracy: 0.8075\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 683us/step - loss: 0.1339 - accuracy: 0.7968\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 692us/step - loss: 0.1269 - accuracy: 0.8342\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.1230 - accuracy: 0.8021\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.1197 - accuracy: 0.8396\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 644us/step - loss: 0.1186 - accuracy: 0.8182\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 672us/step - loss: 0.1217 - accuracy: 0.8128\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 676us/step - loss: 0.1165 - accuracy: 0.8503\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 694us/step - loss: 0.1137 - accuracy: 0.8556\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 679us/step - loss: 0.1089 - accuracy: 0.8449\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.1078 - accuracy: 0.8503\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.1042 - accuracy: 0.8663\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.0994 - accuracy: 0.8610\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 689us/step - loss: 0.1005 - accuracy: 0.8770\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.0947 - accuracy: 0.8930\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.0965 - accuracy: 0.8877\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0908 - accuracy: 0.8770\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.0960 - accuracy: 0.8824\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 665us/step - loss: 0.0901 - accuracy: 0.9037\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.0867 - accuracy: 0.8930\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 691us/step - loss: 0.0823 - accuracy: 0.8930\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 668us/step - loss: 0.0820 - accuracy: 0.8984\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 687us/step - loss: 0.0846 - accuracy: 0.8824\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.0781 - accuracy: 0.8984\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.0784 - accuracy: 0.8984\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.0785 - accuracy: 0.8984\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 684us/step - loss: 0.0747 - accuracy: 0.8984\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 642us/step - loss: 0.0728 - accuracy: 0.9091\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.0710 - accuracy: 0.9251\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.0695 - accuracy: 0.9251\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 683us/step - loss: 0.0689 - accuracy: 0.9251\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 669us/step - loss: 0.0694 - accuracy: 0.9144\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.0671 - accuracy: 0.9251\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.0672 - accuracy: 0.9251\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.0631 - accuracy: 0.9305\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0594 - accuracy: 0.9412\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 692us/step - loss: 0.0609 - accuracy: 0.9358\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.0603 - accuracy: 0.9465\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.0585 - accuracy: 0.9358\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.0611 - accuracy: 0.9305\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.0544 - accuracy: 0.9465\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.0566 - accuracy: 0.9465\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.0528 - accuracy: 0.9412\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.0529 - accuracy: 0.9519\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.0489 - accuracy: 0.9572\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.0472 - accuracy: 0.9572\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.0476 - accuracy: 0.9733\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.0454 - accuracy: 0.9626\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 675us/step - loss: 0.0452 - accuracy: 0.9519\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 694us/step - loss: 0.0456 - accuracy: 0.9679\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 640us/step - loss: 0.0420 - accuracy: 0.9786\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.0419 - accuracy: 0.9679\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.0414 - accuracy: 0.9786\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 679us/step - loss: 0.0388 - accuracy: 0.9733\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 672us/step - loss: 0.0384 - accuracy: 0.9786\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 677us/step - loss: 0.0362 - accuracy: 0.9786\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.0366 - accuracy: 0.9786\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 681us/step - loss: 0.0370 - accuracy: 0.9626\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.0351 - accuracy: 0.9840\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.0350 - accuracy: 0.9786\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 677us/step - loss: 0.0339 - accuracy: 0.9786\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.0342 - accuracy: 0.9679\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.0325 - accuracy: 0.9840\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.0297 - accuracy: 0.9840\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.0294 - accuracy: 0.9840\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 691us/step - loss: 0.0295 - accuracy: 0.9840\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.0276 - accuracy: 0.9840\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0259 - accuracy: 0.9840\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 702us/step - loss: 0.0255 - accuracy: 0.9840\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.0237 - accuracy: 0.9840\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 654us/step - loss: 0.0247 - accuracy: 0.9840\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.0243 - accuracy: 0.9840\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 680us/step - loss: 0.0257 - accuracy: 0.9840\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0228 - accuracy: 0.9840\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.0220 - accuracy: 0.9840\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 655us/step - loss: 0.0238 - accuracy: 0.9786\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.0222 - accuracy: 0.9840\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 677us/step - loss: 0.0214 - accuracy: 0.9840\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0190 - accuracy: 0.9893\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.0192 - accuracy: 0.9840\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.0193 - accuracy: 0.9840\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 673us/step - loss: 0.0212 - accuracy: 0.9840\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 689us/step - loss: 0.0186 - accuracy: 0.9840\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.0177 - accuracy: 0.9893\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 684us/step - loss: 0.0165 - accuracy: 0.9947\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0162 - accuracy: 0.9893\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.0153 - accuracy: 0.9893\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.0151 - accuracy: 0.9840\n",
      "1/1 [==============================] - 0s 504us/step - loss: 0.2439 - accuracy: 0.7143\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.2494 - accuracy: 0.5187\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 586us/step - loss: 0.2394 - accuracy: 0.5775\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 638us/step - loss: 0.2326 - accuracy: 0.6684\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 673us/step - loss: 0.2258 - accuracy: 0.6898\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 617us/step - loss: 0.2177 - accuracy: 0.6578\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 631us/step - loss: 0.2120 - accuracy: 0.6845\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 631us/step - loss: 0.2027 - accuracy: 0.7219\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 637us/step - loss: 0.1944 - accuracy: 0.7433\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 601us/step - loss: 0.1884 - accuracy: 0.7219\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 615us/step - loss: 0.1824 - accuracy: 0.7647\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 601us/step - loss: 0.1787 - accuracy: 0.7273\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 652us/step - loss: 0.1768 - accuracy: 0.7112\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 628us/step - loss: 0.1734 - accuracy: 0.7807\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 646us/step - loss: 0.1645 - accuracy: 0.7487\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 640us/step - loss: 0.1585 - accuracy: 0.7807\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.1547 - accuracy: 0.7647\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.1536 - accuracy: 0.7754\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.1501 - accuracy: 0.7914\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 607us/step - loss: 0.1480 - accuracy: 0.7968\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 647us/step - loss: 0.1494 - accuracy: 0.7914\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 621us/step - loss: 0.1436 - accuracy: 0.7968\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 645us/step - loss: 0.1387 - accuracy: 0.8342\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 637us/step - loss: 0.1364 - accuracy: 0.8182\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.1348 - accuracy: 0.8342\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 654us/step - loss: 0.1331 - accuracy: 0.8182\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 598us/step - loss: 0.1276 - accuracy: 0.8556\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 626us/step - loss: 0.1282 - accuracy: 0.8182\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 639us/step - loss: 0.1302 - accuracy: 0.8128\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 647us/step - loss: 0.1231 - accuracy: 0.8396\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.1273 - accuracy: 0.8289\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 574us/step - loss: 0.1221 - accuracy: 0.8396\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 649us/step - loss: 0.1163 - accuracy: 0.8717\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 646us/step - loss: 0.1117 - accuracy: 0.8610\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 607us/step - loss: 0.1097 - accuracy: 0.8717\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 641us/step - loss: 0.1183 - accuracy: 0.8289\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 619us/step - loss: 0.1055 - accuracy: 0.8610\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 639us/step - loss: 0.1061 - accuracy: 0.8770\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 594us/step - loss: 0.1063 - accuracy: 0.8396\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.1037 - accuracy: 0.8717\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 641us/step - loss: 0.0993 - accuracy: 0.8877\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.1033 - accuracy: 0.8770\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 618us/step - loss: 0.1009 - accuracy: 0.8663\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0926 - accuracy: 0.8877\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 640us/step - loss: 0.1025 - accuracy: 0.8342\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 634us/step - loss: 0.0954 - accuracy: 0.8770\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 643us/step - loss: 0.0979 - accuracy: 0.8503\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 644us/step - loss: 0.0923 - accuracy: 0.8930\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 664us/step - loss: 0.0895 - accuracy: 0.8930\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0877 - accuracy: 0.8877\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 657us/step - loss: 0.0876 - accuracy: 0.8984\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 626us/step - loss: 0.0879 - accuracy: 0.8930\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 612us/step - loss: 0.0878 - accuracy: 0.8984\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0843 - accuracy: 0.8877\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.0869 - accuracy: 0.8824\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 589us/step - loss: 0.0804 - accuracy: 0.9091\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 646us/step - loss: 0.0791 - accuracy: 0.9037\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 670us/step - loss: 0.0815 - accuracy: 0.8770\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 610us/step - loss: 0.0785 - accuracy: 0.9144\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 641us/step - loss: 0.0764 - accuracy: 0.9305\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 604us/step - loss: 0.0768 - accuracy: 0.8930\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 628us/step - loss: 0.0747 - accuracy: 0.9198\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.0766 - accuracy: 0.9091\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 656us/step - loss: 0.0769 - accuracy: 0.8930\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 653us/step - loss: 0.0765 - accuracy: 0.9144\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 652us/step - loss: 0.0672 - accuracy: 0.9358\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 611us/step - loss: 0.0689 - accuracy: 0.9144\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.0672 - accuracy: 0.9305\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0690 - accuracy: 0.9465\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.0646 - accuracy: 0.9412\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0711 - accuracy: 0.9037\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0659 - accuracy: 0.9412\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 607us/step - loss: 0.0654 - accuracy: 0.9465\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 618us/step - loss: 0.0633 - accuracy: 0.9412\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 621us/step - loss: 0.0596 - accuracy: 0.9358\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0584 - accuracy: 0.9572\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 594us/step - loss: 0.0577 - accuracy: 0.9412\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 635us/step - loss: 0.0572 - accuracy: 0.9465\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 579us/step - loss: 0.0616 - accuracy: 0.9412\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 626us/step - loss: 0.0546 - accuracy: 0.9519\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 648us/step - loss: 0.0514 - accuracy: 0.9679\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 628us/step - loss: 0.0586 - accuracy: 0.9412\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.0511 - accuracy: 0.9679\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 618us/step - loss: 0.0489 - accuracy: 0.9679\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 638us/step - loss: 0.0517 - accuracy: 0.9572\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.0475 - accuracy: 0.9733\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 596us/step - loss: 0.0551 - accuracy: 0.9519\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 659us/step - loss: 0.0472 - accuracy: 0.9679\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 622us/step - loss: 0.0434 - accuracy: 0.9679\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.0540 - accuracy: 0.9412\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 645us/step - loss: 0.0453 - accuracy: 0.9519\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 617us/step - loss: 0.0427 - accuracy: 0.9733\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 646us/step - loss: 0.0425 - accuracy: 0.9679\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 598us/step - loss: 0.0419 - accuracy: 0.9572\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.0402 - accuracy: 0.9733\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 586us/step - loss: 0.0418 - accuracy: 0.9786\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 609us/step - loss: 0.0393 - accuracy: 0.9786\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 598us/step - loss: 0.0386 - accuracy: 0.9733\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 628us/step - loss: 0.0382 - accuracy: 0.9733\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 598us/step - loss: 0.0359 - accuracy: 0.9786\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 636us/step - loss: 0.0413 - accuracy: 0.9679\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.8095\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.2479 - accuracy: 0.5348\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 609us/step - loss: 0.2418 - accuracy: 0.5348\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 659us/step - loss: 0.2359 - accuracy: 0.5615\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.2311 - accuracy: 0.5882\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 687us/step - loss: 0.2259 - accuracy: 0.5775\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.2230 - accuracy: 0.6578\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.2163 - accuracy: 0.6364\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 668us/step - loss: 0.2098 - accuracy: 0.7112\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.1991 - accuracy: 0.7273\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.1893 - accuracy: 0.7754\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 679us/step - loss: 0.1815 - accuracy: 0.7487\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.1782 - accuracy: 0.7647\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.1711 - accuracy: 0.7594\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.1610 - accuracy: 0.7861\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 658us/step - loss: 0.1542 - accuracy: 0.7914\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 668us/step - loss: 0.1486 - accuracy: 0.8289\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 688us/step - loss: 0.1448 - accuracy: 0.8075\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 680us/step - loss: 0.1414 - accuracy: 0.8182\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.1398 - accuracy: 0.8182\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 688us/step - loss: 0.1366 - accuracy: 0.8235\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 669us/step - loss: 0.1359 - accuracy: 0.7968\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.1282 - accuracy: 0.8503\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.1264 - accuracy: 0.8235\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.1264 - accuracy: 0.8503\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 663us/step - loss: 0.1197 - accuracy: 0.8503\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.1226 - accuracy: 0.8396\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.1145 - accuracy: 0.8396\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.1159 - accuracy: 0.8342\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.1098 - accuracy: 0.8610\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.1110 - accuracy: 0.8717\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.1059 - accuracy: 0.8824\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.1034 - accuracy: 0.8824\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 692us/step - loss: 0.1016 - accuracy: 0.8877\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.0980 - accuracy: 0.9037\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 665us/step - loss: 0.1025 - accuracy: 0.8824\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.0926 - accuracy: 0.8930\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0930 - accuracy: 0.8663\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.1005 - accuracy: 0.8770\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 725us/step - loss: 0.0925 - accuracy: 0.8770\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 676us/step - loss: 0.0906 - accuracy: 0.8984\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0878 - accuracy: 0.8984\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.0853 - accuracy: 0.9037\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.0837 - accuracy: 0.8930\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.0898 - accuracy: 0.8930\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.0815 - accuracy: 0.9198\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.0853 - accuracy: 0.8984\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 691us/step - loss: 0.0794 - accuracy: 0.9037\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.0745 - accuracy: 0.9198\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.0716 - accuracy: 0.9251\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.0725 - accuracy: 0.9037\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0701 - accuracy: 0.9305\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 685us/step - loss: 0.0707 - accuracy: 0.9251\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.0695 - accuracy: 0.9091\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.0668 - accuracy: 0.9251\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.0624 - accuracy: 0.9251\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.0669 - accuracy: 0.9305\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.0590 - accuracy: 0.9358\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 689us/step - loss: 0.0634 - accuracy: 0.9412\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.0575 - accuracy: 0.9412\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.0601 - accuracy: 0.9358\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.0539 - accuracy: 0.9358\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 688us/step - loss: 0.0569 - accuracy: 0.9465\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0532 - accuracy: 0.9465\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.0536 - accuracy: 0.9412\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.0513 - accuracy: 0.9465\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.0471 - accuracy: 0.9465\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.0481 - accuracy: 0.9519\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.0471 - accuracy: 0.9519\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 685us/step - loss: 0.0453 - accuracy: 0.9626\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 676us/step - loss: 0.0480 - accuracy: 0.9626\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.0419 - accuracy: 0.9626\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.0500 - accuracy: 0.9465\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.0435 - accuracy: 0.9626\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.0404 - accuracy: 0.9679\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 681us/step - loss: 0.0395 - accuracy: 0.9786\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 694us/step - loss: 0.0379 - accuracy: 0.9786\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.0369 - accuracy: 0.9679\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 687us/step - loss: 0.0355 - accuracy: 0.9679\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.0359 - accuracy: 0.9679\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.0340 - accuracy: 0.9733\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 666us/step - loss: 0.0345 - accuracy: 0.9733\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 659us/step - loss: 0.0341 - accuracy: 0.9840\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 654us/step - loss: 0.0308 - accuracy: 0.9733\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.0307 - accuracy: 0.9840\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0313 - accuracy: 0.9786\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.0360 - accuracy: 0.9679\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.0300 - accuracy: 0.9733\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 683us/step - loss: 0.0284 - accuracy: 0.9786\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.0274 - accuracy: 0.9786\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.0269 - accuracy: 0.9786\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.0273 - accuracy: 0.9840\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 677us/step - loss: 0.0265 - accuracy: 0.9786\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.0271 - accuracy: 0.9840\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.0259 - accuracy: 0.9786\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 662us/step - loss: 0.0246 - accuracy: 0.9786\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 694us/step - loss: 0.0228 - accuracy: 0.9893\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0236 - accuracy: 0.9840\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 687us/step - loss: 0.0255 - accuracy: 0.9786\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 653us/step - loss: 0.0217 - accuracy: 0.9893\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 680us/step - loss: 0.0211 - accuracy: 0.9786\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000199F9DC1708> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.1972 - accuracy: 0.7619\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 626us/step - loss: 0.2516 - accuracy: 0.5134\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 632us/step - loss: 0.2400 - accuracy: 0.5989\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.2329 - accuracy: 0.6738\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 661us/step - loss: 0.2261 - accuracy: 0.7219\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 598us/step - loss: 0.2177 - accuracy: 0.7059\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 586us/step - loss: 0.2092 - accuracy: 0.7059\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 603us/step - loss: 0.1988 - accuracy: 0.7594\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 596us/step - loss: 0.1883 - accuracy: 0.8021\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.1791 - accuracy: 0.8021\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.1693 - accuracy: 0.8075\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 593us/step - loss: 0.1632 - accuracy: 0.8235\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 579us/step - loss: 0.1644 - accuracy: 0.8021\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 628us/step - loss: 0.1598 - accuracy: 0.7701\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 610us/step - loss: 0.1486 - accuracy: 0.8128\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 651us/step - loss: 0.1390 - accuracy: 0.8663\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 589us/step - loss: 0.1344 - accuracy: 0.8396\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.1311 - accuracy: 0.8449\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 601us/step - loss: 0.1281 - accuracy: 0.8503\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 581us/step - loss: 0.1218 - accuracy: 0.8556\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 592us/step - loss: 0.1284 - accuracy: 0.8396\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 626us/step - loss: 0.1200 - accuracy: 0.8503\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.1143 - accuracy: 0.8556\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.1100 - accuracy: 0.8824\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 595us/step - loss: 0.1103 - accuracy: 0.8663\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 572us/step - loss: 0.1105 - accuracy: 0.8717\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.1023 - accuracy: 0.8824\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.1043 - accuracy: 0.8984\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.1019 - accuracy: 0.8770\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 558us/step - loss: 0.0986 - accuracy: 0.8877\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.1108 - accuracy: 0.8556\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 562us/step - loss: 0.1032 - accuracy: 0.8503\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 566us/step - loss: 0.0953 - accuracy: 0.8770\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.0893 - accuracy: 0.8930\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 577us/step - loss: 0.0860 - accuracy: 0.8984\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 576us/step - loss: 0.0915 - accuracy: 0.8717\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 601us/step - loss: 0.0798 - accuracy: 0.9037\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 581us/step - loss: 0.0828 - accuracy: 0.8984\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 621us/step - loss: 0.0814 - accuracy: 0.9037\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.0793 - accuracy: 0.9037\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 633us/step - loss: 0.0764 - accuracy: 0.9144\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 574us/step - loss: 0.0772 - accuracy: 0.9091\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 571us/step - loss: 0.0750 - accuracy: 0.8984\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 599us/step - loss: 0.0722 - accuracy: 0.9144\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 616us/step - loss: 0.0732 - accuracy: 0.9144\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 591us/step - loss: 0.0691 - accuracy: 0.9198\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 578us/step - loss: 0.0695 - accuracy: 0.9198\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 632us/step - loss: 0.0678 - accuracy: 0.9144\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0693 - accuracy: 0.9144\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 597us/step - loss: 0.0644 - accuracy: 0.9358\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.0625 - accuracy: 0.9251\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.0600 - accuracy: 0.9305\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.0556 - accuracy: 0.9358\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 613us/step - loss: 0.0583 - accuracy: 0.9251\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 626us/step - loss: 0.0561 - accuracy: 0.9358\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.0514 - accuracy: 0.9572\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.0551 - accuracy: 0.9465\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 599us/step - loss: 0.0483 - accuracy: 0.9679\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0496 - accuracy: 0.9572\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.0488 - accuracy: 0.9572\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 575us/step - loss: 0.0478 - accuracy: 0.9465\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.0479 - accuracy: 0.9572\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 595us/step - loss: 0.0446 - accuracy: 0.9572\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 586us/step - loss: 0.0468 - accuracy: 0.9412\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 633us/step - loss: 0.0423 - accuracy: 0.9679\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 587us/step - loss: 0.0426 - accuracy: 0.9465\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 614us/step - loss: 0.0376 - accuracy: 0.9733\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 574us/step - loss: 0.0358 - accuracy: 0.9733\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 618us/step - loss: 0.0343 - accuracy: 0.9786\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.0370 - accuracy: 0.9572\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.0357 - accuracy: 0.9572\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 562us/step - loss: 0.0319 - accuracy: 0.9786\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.0322 - accuracy: 0.9786\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 578us/step - loss: 0.0299 - accuracy: 0.9786\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 598us/step - loss: 0.0310 - accuracy: 0.9786\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 585us/step - loss: 0.0280 - accuracy: 0.9840\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 614us/step - loss: 0.0268 - accuracy: 0.9786\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.0240 - accuracy: 0.9840\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 611us/step - loss: 0.0259 - accuracy: 0.9840\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.0241 - accuracy: 0.9840\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 580us/step - loss: 0.0226 - accuracy: 0.9893\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0246 - accuracy: 0.9893\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.0222 - accuracy: 0.9893\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 593us/step - loss: 0.0201 - accuracy: 0.9947\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 577us/step - loss: 0.0189 - accuracy: 0.9947\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 592us/step - loss: 0.0230 - accuracy: 0.9733\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 593us/step - loss: 0.0195 - accuracy: 0.9893\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 571us/step - loss: 0.0204 - accuracy: 0.9893\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 579us/step - loss: 0.0161 - accuracy: 0.9947\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 616us/step - loss: 0.0162 - accuracy: 0.9947\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 1.00 - 0s 633us/step - loss: 0.0141 - accuracy: 0.9947\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 617us/step - loss: 0.0182 - accuracy: 0.9893\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 641us/step - loss: 0.0181 - accuracy: 0.9947\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 638us/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 621us/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 579us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x0000019981513678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.9048\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 626us/step - loss: 0.2532 - accuracy: 0.4385\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 639us/step - loss: 0.2421 - accuracy: 0.5668\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 669us/step - loss: 0.2298 - accuracy: 0.6631\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 683us/step - loss: 0.2198 - accuracy: 0.7219\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.2087 - accuracy: 0.6952\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 633us/step - loss: 0.1962 - accuracy: 0.7701\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 596us/step - loss: 0.1868 - accuracy: 0.7594\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 612us/step - loss: 0.1731 - accuracy: 0.8075\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.1619 - accuracy: 0.8128\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.1535 - accuracy: 0.8182\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.1469 - accuracy: 0.8235\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 601us/step - loss: 0.1463 - accuracy: 0.7754\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 580us/step - loss: 0.1379 - accuracy: 0.8075\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.1301 - accuracy: 0.8396\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 603us/step - loss: 0.1262 - accuracy: 0.8610\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.1226 - accuracy: 0.8663\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 599us/step - loss: 0.1222 - accuracy: 0.8503\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 607us/step - loss: 0.1140 - accuracy: 0.8663\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.1168 - accuracy: 0.8342\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.1229 - accuracy: 0.8396\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 602us/step - loss: 0.1119 - accuracy: 0.8663\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 615us/step - loss: 0.1047 - accuracy: 0.8930\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.0997 - accuracy: 0.8930\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 581us/step - loss: 0.1039 - accuracy: 0.8503\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 602us/step - loss: 0.0995 - accuracy: 0.8824\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 612us/step - loss: 0.0950 - accuracy: 0.8930\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 631us/step - loss: 0.0937 - accuracy: 0.8824\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0883 - accuracy: 0.8877\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.0860 - accuracy: 0.9091\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 652us/step - loss: 0.0843 - accuracy: 0.9037\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 672us/step - loss: 0.0802 - accuracy: 0.9251\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 619us/step - loss: 0.0782 - accuracy: 0.8984\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0758 - accuracy: 0.9091\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 663us/step - loss: 0.0759 - accuracy: 0.9305\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0746 - accuracy: 0.9198\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 1.00 - 0s 662us/step - loss: 0.0645 - accuracy: 0.9251\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.0689 - accuracy: 0.9198\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 621us/step - loss: 0.0625 - accuracy: 0.9465\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 649us/step - loss: 0.0613 - accuracy: 0.9465\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 650us/step - loss: 0.0585 - accuracy: 0.9465\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 648us/step - loss: 0.0599 - accuracy: 0.9358\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 634us/step - loss: 0.0576 - accuracy: 0.9358\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.0569 - accuracy: 0.9519\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 660us/step - loss: 0.0554 - accuracy: 0.9519\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 673us/step - loss: 0.0454 - accuracy: 0.9679\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 622us/step - loss: 0.0503 - accuracy: 0.9519\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 651us/step - loss: 0.0465 - accuracy: 0.9679\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 615us/step - loss: 0.0424 - accuracy: 0.9626\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 632us/step - loss: 0.0400 - accuracy: 0.9679\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 656us/step - loss: 0.0388 - accuracy: 0.9626\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 640us/step - loss: 0.0389 - accuracy: 0.9572\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 622us/step - loss: 0.0350 - accuracy: 0.9733\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.0344 - accuracy: 0.9786\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.0348 - accuracy: 0.9840\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 643us/step - loss: 0.0329 - accuracy: 0.9733\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 641us/step - loss: 0.0297 - accuracy: 0.9786\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 636us/step - loss: 0.0255 - accuracy: 0.9947\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.0258 - accuracy: 0.9893\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 628us/step - loss: 0.0255 - accuracy: 0.9893\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 602us/step - loss: 0.0202 - accuracy: 0.9947\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 652us/step - loss: 0.0198 - accuracy: 0.9947\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 653us/step - loss: 0.0180 - accuracy: 0.9947\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 633us/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0170 - accuracy: 0.9947\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 617us/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 615us/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 650us/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 622us/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 657us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 655us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 643us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 611us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 618us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 637us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 609us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 631us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 622us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 631us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 645us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 586us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 638us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 642us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 607us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "WARNING:tensorflow:7 out of the last 15 calls to <function Model.make_test_function.<locals>.test_function at 0x00000199FFA2CA68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.8571\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 640us/step - loss: 0.2446 - accuracy: 0.4920\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 555us/step - loss: 0.2348 - accuracy: 0.6364\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 657us/step - loss: 0.2262 - accuracy: 0.6738\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.2115 - accuracy: 0.7219\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 657us/step - loss: 0.1915 - accuracy: 0.7487\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 603us/step - loss: 0.1833 - accuracy: 0.7540\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 597us/step - loss: 0.1695 - accuracy: 0.7807\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 648us/step - loss: 0.1601 - accuracy: 0.8182\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 607us/step - loss: 0.1527 - accuracy: 0.8182\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.1466 - accuracy: 0.7914\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 640us/step - loss: 0.1437 - accuracy: 0.8235\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.1427 - accuracy: 0.8075\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 619us/step - loss: 0.1380 - accuracy: 0.8182\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.1304 - accuracy: 0.8182\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 576us/step - loss: 0.1255 - accuracy: 0.8182\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 615us/step - loss: 0.1211 - accuracy: 0.8717\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 603us/step - loss: 0.1172 - accuracy: 0.8556\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.1179 - accuracy: 0.8503\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 651us/step - loss: 0.1152 - accuracy: 0.8556\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.40 - 0s 606us/step - loss: 0.1226 - accuracy: 0.8235\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 601us/step - loss: 0.1170 - accuracy: 0.8396\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 585us/step - loss: 0.1047 - accuracy: 0.8824\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 603us/step - loss: 0.1023 - accuracy: 0.8770\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.1017 - accuracy: 0.8663\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 644us/step - loss: 0.0965 - accuracy: 0.8824\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0991 - accuracy: 0.8770\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 636us/step - loss: 0.0935 - accuracy: 0.8930\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0955 - accuracy: 0.9037\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 602us/step - loss: 0.0877 - accuracy: 0.9198\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 611us/step - loss: 0.0932 - accuracy: 0.8824\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 599us/step - loss: 0.0870 - accuracy: 0.8824\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 592us/step - loss: 0.0855 - accuracy: 0.9144\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 654us/step - loss: 0.0850 - accuracy: 0.9037\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 602us/step - loss: 0.0829 - accuracy: 0.9198\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 610us/step - loss: 0.0815 - accuracy: 0.9144\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 647us/step - loss: 0.0768 - accuracy: 0.9251\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.0842 - accuracy: 0.9091\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 640us/step - loss: 0.0801 - accuracy: 0.9144\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 635us/step - loss: 0.0736 - accuracy: 0.9305\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 642us/step - loss: 0.0701 - accuracy: 0.9412\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.0717 - accuracy: 0.9198\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 591us/step - loss: 0.0671 - accuracy: 0.9412\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0706 - accuracy: 0.9358\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0675 - accuracy: 0.9198\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 651us/step - loss: 0.0667 - accuracy: 0.9305\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 633us/step - loss: 0.0651 - accuracy: 0.9358\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 614us/step - loss: 0.0630 - accuracy: 0.9465\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 611us/step - loss: 0.0606 - accuracy: 0.9465\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 596us/step - loss: 0.0605 - accuracy: 0.9358\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0582 - accuracy: 0.9412\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.0669 - accuracy: 0.9358\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.0571 - accuracy: 0.9358\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 649us/step - loss: 0.0564 - accuracy: 0.9572\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 614us/step - loss: 0.0563 - accuracy: 0.9305\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 638us/step - loss: 0.0576 - accuracy: 0.9465\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 650us/step - loss: 0.0520 - accuracy: 0.9572\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 631us/step - loss: 0.0497 - accuracy: 0.9412\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 655us/step - loss: 0.0510 - accuracy: 0.9519\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 587us/step - loss: 0.0506 - accuracy: 0.9465\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 656us/step - loss: 0.0479 - accuracy: 0.9572\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.0453 - accuracy: 0.9519\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 657us/step - loss: 0.0445 - accuracy: 0.9679\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 626us/step - loss: 0.0429 - accuracy: 0.9679\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 588us/step - loss: 0.0425 - accuracy: 0.9679\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 633us/step - loss: 0.0420 - accuracy: 0.9679\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 588us/step - loss: 0.0402 - accuracy: 0.9733\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 603us/step - loss: 0.0381 - accuracy: 0.9626\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 628us/step - loss: 0.0413 - accuracy: 0.9626\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0380 - accuracy: 0.9786\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 587us/step - loss: 0.0390 - accuracy: 0.9465\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 638us/step - loss: 0.0352 - accuracy: 0.9733\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 615us/step - loss: 0.0336 - accuracy: 0.9840\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 592us/step - loss: 0.0330 - accuracy: 0.9679\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 598us/step - loss: 0.0338 - accuracy: 0.9733\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 614us/step - loss: 0.0361 - accuracy: 0.9733\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.0314 - accuracy: 0.9893\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0304 - accuracy: 0.9893\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0277 - accuracy: 0.9840\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 615us/step - loss: 0.0294 - accuracy: 0.9840\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0282 - accuracy: 0.9840\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0260 - accuracy: 0.9893\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 597us/step - loss: 0.0249 - accuracy: 0.9893\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 647us/step - loss: 0.0254 - accuracy: 0.9893\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0248 - accuracy: 0.9893\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0266 - accuracy: 0.9893\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0242 - accuracy: 0.9840\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 611us/step - loss: 0.0229 - accuracy: 0.9893\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 649us/step - loss: 0.0245 - accuracy: 0.9893\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 637us/step - loss: 0.0250 - accuracy: 0.9840\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0233 - accuracy: 0.9893\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 577us/step - loss: 0.0236 - accuracy: 0.9893\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 614us/step - loss: 0.0211 - accuracy: 0.9893\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 611us/step - loss: 0.0210 - accuracy: 0.9893\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.0198 - accuracy: 0.9893\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 643us/step - loss: 0.0180 - accuracy: 0.9893\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 596us/step - loss: 0.0177 - accuracy: 0.9893\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0201 - accuracy: 0.9893\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 655us/step - loss: 0.0178 - accuracy: 0.9893\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.0185 - accuracy: 0.9893\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 642us/step - loss: 0.0170 - accuracy: 0.9893\n",
      "WARNING:tensorflow:8 out of the last 16 calls to <function Model.make_test_function.<locals>.test_function at 0x000001998029C558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.7619\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 657us/step - loss: 0.2412 - accuracy: 0.5668\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 574us/step - loss: 0.2277 - accuracy: 0.6524\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 667us/step - loss: 0.2170 - accuracy: 0.6791\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 622us/step - loss: 0.2086 - accuracy: 0.7166\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 576us/step - loss: 0.1996 - accuracy: 0.7112\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 628us/step - loss: 0.1916 - accuracy: 0.7807\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.1820 - accuracy: 0.7701\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 561us/step - loss: 0.1716 - accuracy: 0.7968\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 574us/step - loss: 0.1643 - accuracy: 0.8021\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 594us/step - loss: 0.1585 - accuracy: 0.8075\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.1526 - accuracy: 0.8235\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 621us/step - loss: 0.1542 - accuracy: 0.7861\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 584us/step - loss: 0.1503 - accuracy: 0.7861\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 576us/step - loss: 0.1387 - accuracy: 0.8021\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.1352 - accuracy: 0.8342\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 559us/step - loss: 0.1304 - accuracy: 0.8289\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 578us/step - loss: 0.1254 - accuracy: 0.8449\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.1248 - accuracy: 0.8449\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 587us/step - loss: 0.1243 - accuracy: 0.8182\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 579us/step - loss: 0.1228 - accuracy: 0.8503\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.1163 - accuracy: 0.8556\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 589us/step - loss: 0.1138 - accuracy: 0.8556\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 616us/step - loss: 0.1096 - accuracy: 0.8610\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 599us/step - loss: 0.1111 - accuracy: 0.8610\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 586us/step - loss: 0.1036 - accuracy: 0.8717\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 557us/step - loss: 0.1015 - accuracy: 0.8770\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.0986 - accuracy: 0.8717\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0977 - accuracy: 0.8877\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 573us/step - loss: 0.0946 - accuracy: 0.8717\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.0963 - accuracy: 0.8770\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 566us/step - loss: 0.0954 - accuracy: 0.8396\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 567us/step - loss: 0.0915 - accuracy: 0.8930\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 593us/step - loss: 0.0860 - accuracy: 0.8824\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 568us/step - loss: 0.0858 - accuracy: 0.8824\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0964 - accuracy: 0.8556\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 641us/step - loss: 0.0782 - accuracy: 0.9037\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 592us/step - loss: 0.0838 - accuracy: 0.8770\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 572us/step - loss: 0.0812 - accuracy: 0.9037\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 589us/step - loss: 0.0766 - accuracy: 0.9198\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.0756 - accuracy: 0.9037\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 599us/step - loss: 0.0790 - accuracy: 0.8877\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.0710 - accuracy: 0.9198\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 597us/step - loss: 0.0686 - accuracy: 0.9465\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 612us/step - loss: 0.0729 - accuracy: 0.9037\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.0653 - accuracy: 0.9305\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 622us/step - loss: 0.0672 - accuracy: 0.9305\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 597us/step - loss: 0.0634 - accuracy: 0.9572\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0587 - accuracy: 0.9412\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 587us/step - loss: 0.0601 - accuracy: 0.9305\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 576us/step - loss: 0.0590 - accuracy: 0.9251\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.0546 - accuracy: 0.9519\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 613us/step - loss: 0.0576 - accuracy: 0.9358\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 577us/step - loss: 0.0596 - accuracy: 0.9305\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 579us/step - loss: 0.0535 - accuracy: 0.9412\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.0492 - accuracy: 0.9519\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 567us/step - loss: 0.0482 - accuracy: 0.9679\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 577us/step - loss: 0.0472 - accuracy: 0.9572\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 578us/step - loss: 0.0491 - accuracy: 0.9626\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 597us/step - loss: 0.0472 - accuracy: 0.9572\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 553us/step - loss: 0.0426 - accuracy: 0.9626\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 582us/step - loss: 0.0465 - accuracy: 0.9465\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 598us/step - loss: 0.0409 - accuracy: 0.9626\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 577us/step - loss: 0.0393 - accuracy: 0.9786\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 577us/step - loss: 0.0396 - accuracy: 0.9679\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 598us/step - loss: 0.0401 - accuracy: 0.9733\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 563us/step - loss: 0.0362 - accuracy: 0.9786\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 595us/step - loss: 0.0334 - accuracy: 0.9733\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 565us/step - loss: 0.0335 - accuracy: 0.9733\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 623us/step - loss: 0.0333 - accuracy: 0.9679\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 592us/step - loss: 0.0367 - accuracy: 0.9626\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 635us/step - loss: 0.0304 - accuracy: 0.9786\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 595us/step - loss: 0.0341 - accuracy: 0.9626\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 607us/step - loss: 0.0281 - accuracy: 0.9840\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 574us/step - loss: 0.0273 - accuracy: 0.9893\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 571us/step - loss: 0.0285 - accuracy: 0.9786\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 602us/step - loss: 0.0274 - accuracy: 0.9840\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 597us/step - loss: 0.0262 - accuracy: 0.9786\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 593us/step - loss: 0.0238 - accuracy: 0.9893\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 638us/step - loss: 0.0239 - accuracy: 0.9893\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 586us/step - loss: 0.0247 - accuracy: 0.9893\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 579us/step - loss: 0.0237 - accuracy: 0.9947\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 612us/step - loss: 0.0196 - accuracy: 0.9947\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 588us/step - loss: 0.0189 - accuracy: 0.9947\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0200 - accuracy: 0.9893\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 574us/step - loss: 0.0220 - accuracy: 0.9893\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 596us/step - loss: 0.0207 - accuracy: 0.9840\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 591us/step - loss: 0.0170 - accuracy: 0.9893\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.0170 - accuracy: 0.9893\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 575us/step - loss: 0.0193 - accuracy: 0.9893\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0167 - accuracy: 0.9893\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 576us/step - loss: 0.0154 - accuracy: 0.9893\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 577us/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 536us/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 602us/step - loss: 0.0158 - accuracy: 0.9893\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0134 - accuracy: 0.9947\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 566us/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 588us/step - loss: 0.0125 - accuracy: 0.9947\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 584us/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 597us/step - loss: 0.0104 - accuracy: 1.0000\n",
      "WARNING:tensorflow:9 out of the last 17 calls to <function Model.make_test_function.<locals>.test_function at 0x000001998029CD38> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0562 - accuracy: 0.8571\n",
      "Epoch 1/100\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.2274 - accuracy: 0.6000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "38/38 [==============================] - 0s 661us/step - loss: 0.2433 - accuracy: 0.5294\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 593us/step - loss: 0.2237 - accuracy: 0.6631\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 593us/step - loss: 0.2093 - accuracy: 0.7540\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1932 - accuracy: 0.7594\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 606us/step - loss: 0.1809 - accuracy: 0.7433\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 653us/step - loss: 0.1719 - accuracy: 0.7540\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 621us/step - loss: 0.1619 - accuracy: 0.7968\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 599us/step - loss: 0.1545 - accuracy: 0.7914\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 613us/step - loss: 0.1478 - accuracy: 0.8075\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 667us/step - loss: 0.1431 - accuracy: 0.8449\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 658us/step - loss: 0.1399 - accuracy: 0.8289\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.1417 - accuracy: 0.8182\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 648us/step - loss: 0.1367 - accuracy: 0.8075\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 632us/step - loss: 0.1284 - accuracy: 0.8342\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 648us/step - loss: 0.1236 - accuracy: 0.8396\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 591us/step - loss: 0.1228 - accuracy: 0.8556\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 617us/step - loss: 0.1260 - accuracy: 0.8396\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.1190 - accuracy: 0.8449\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 596us/step - loss: 0.1224 - accuracy: 0.8075\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.1172 - accuracy: 0.8610\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 649us/step - loss: 0.1150 - accuracy: 0.8503\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.1107 - accuracy: 0.8556\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.1044 - accuracy: 0.8930\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 599us/step - loss: 0.1084 - accuracy: 0.8877\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 584us/step - loss: 0.1050 - accuracy: 0.8663\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 595us/step - loss: 0.0969 - accuracy: 0.8824\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0951 - accuracy: 0.8824\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 605us/step - loss: 0.0941 - accuracy: 0.8770\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 597us/step - loss: 0.0943 - accuracy: 0.8930\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 627us/step - loss: 0.0923 - accuracy: 0.8930\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 628us/step - loss: 0.0879 - accuracy: 0.9091\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 652us/step - loss: 0.0829 - accuracy: 0.9251\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 624us/step - loss: 0.0833 - accuracy: 0.9037\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 632us/step - loss: 0.0838 - accuracy: 0.9037\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 644us/step - loss: 0.0851 - accuracy: 0.9037\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 607us/step - loss: 0.0778 - accuracy: 0.9091\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 600us/step - loss: 0.0801 - accuracy: 0.9037\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 631us/step - loss: 0.0744 - accuracy: 0.8984\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 617us/step - loss: 0.0732 - accuracy: 0.9251\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.0695 - accuracy: 0.9251\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 621us/step - loss: 0.0743 - accuracy: 0.9251\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 629us/step - loss: 0.0687 - accuracy: 0.9198\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 608us/step - loss: 0.0650 - accuracy: 0.9358\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.0691 - accuracy: 0.9037\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 611us/step - loss: 0.0642 - accuracy: 0.9198\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 604us/step - loss: 0.0682 - accuracy: 0.9144\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 639us/step - loss: 0.0651 - accuracy: 0.9198\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 654us/step - loss: 0.0630 - accuracy: 0.9358\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 620us/step - loss: 0.0584 - accuracy: 0.9358\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.0582 - accuracy: 0.9251\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 641us/step - loss: 0.0568 - accuracy: 0.9305\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 633us/step - loss: 0.0554 - accuracy: 0.9465\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 647us/step - loss: 0.0535 - accuracy: 0.9358\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 609us/step - loss: 0.0548 - accuracy: 0.9412\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 596us/step - loss: 0.0510 - accuracy: 0.9465\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.0511 - accuracy: 0.9465\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 640us/step - loss: 0.0495 - accuracy: 0.9358\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 630us/step - loss: 0.0501 - accuracy: 0.9519\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 625us/step - loss: 0.0511 - accuracy: 0.9358\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 641us/step - loss: 0.0581 - accuracy: 0.9251\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 671us/step - loss: 0.0501 - accuracy: 0.9412\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 684us/step - loss: 0.0435 - accuracy: 0.9679\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 667us/step - loss: 0.0508 - accuracy: 0.9358\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.0428 - accuracy: 0.9679\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.0413 - accuracy: 0.9679\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 677us/step - loss: 0.0367 - accuracy: 0.9733\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.0426 - accuracy: 0.9626\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.0396 - accuracy: 0.9626\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 659us/step - loss: 0.0398 - accuracy: 0.9679\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.0580 - accuracy: 0.9251\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.0380 - accuracy: 0.9733\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.0362 - accuracy: 0.9679\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.0364 - accuracy: 0.9786\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.0358 - accuracy: 0.9733\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.0349 - accuracy: 0.9786\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.0363 - accuracy: 0.9679\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.0370 - accuracy: 0.9626\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.0320 - accuracy: 0.9786\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 683us/step - loss: 0.0312 - accuracy: 0.9786\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.0316 - accuracy: 0.9786\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.0320 - accuracy: 0.9733\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.0311 - accuracy: 0.9786\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.0288 - accuracy: 0.9786\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 671us/step - loss: 0.0291 - accuracy: 0.9679\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.0278 - accuracy: 0.9786\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.0292 - accuracy: 0.9840\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.0285 - accuracy: 0.9786\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.0260 - accuracy: 0.9840\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.0266 - accuracy: 0.9786\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.0272 - accuracy: 0.9840\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0262 - accuracy: 0.9840\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 661us/step - loss: 0.0277 - accuracy: 0.9733\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.0234 - accuracy: 0.9840\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 689us/step - loss: 0.0240 - accuracy: 0.9786\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.0218 - accuracy: 0.9840\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 658us/step - loss: 0.0230 - accuracy: 0.9840\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 681us/step - loss: 0.0233 - accuracy: 0.9786\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 667us/step - loss: 0.0238 - accuracy: 0.9840\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 679us/step - loss: 0.0214 - accuracy: 0.9840\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.0228 - accuracy: 0.9840\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000199FFEAD1F8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.1386 - accuracy: 0.7619\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 680us/step - loss: 0.2536 - accuracy: 0.4840\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.2447 - accuracy: 0.5426\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 639us/step - loss: 0.2395 - accuracy: 0.5957\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.2308 - accuracy: 0.6915\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.2232 - accuracy: 0.7340\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.2161 - accuracy: 0.6915\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 680us/step - loss: 0.2054 - accuracy: 0.7128\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.1975 - accuracy: 0.7500\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.1907 - accuracy: 0.7660\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.1825 - accuracy: 0.7713\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 678us/step - loss: 0.1816 - accuracy: 0.7234\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.1697 - accuracy: 0.7819\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 702us/step - loss: 0.1652 - accuracy: 0.7713\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.1571 - accuracy: 0.7979\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.1569 - accuracy: 0.7979\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 680us/step - loss: 0.1511 - accuracy: 0.8191\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.1510 - accuracy: 0.7819\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 694us/step - loss: 0.1520 - accuracy: 0.7766\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 706us/step - loss: 0.1433 - accuracy: 0.8085\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 685us/step - loss: 0.1421 - accuracy: 0.8191\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.1414 - accuracy: 0.7926\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.1314 - accuracy: 0.8404\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.1296 - accuracy: 0.8351\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 669us/step - loss: 0.1279 - accuracy: 0.8457\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.1279 - accuracy: 0.8351\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 712us/step - loss: 0.1257 - accuracy: 0.8670\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.1258 - accuracy: 0.8564\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.1206 - accuracy: 0.8830\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.1206 - accuracy: 0.8511\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 714us/step - loss: 0.1239 - accuracy: 0.8511\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.1217 - accuracy: 0.8351\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.1177 - accuracy: 0.8511\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.1123 - accuracy: 0.8830\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 694us/step - loss: 0.1107 - accuracy: 0.8777\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.1115 - accuracy: 0.8777\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.1103 - accuracy: 0.8617\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 684us/step - loss: 0.1076 - accuracy: 0.8723\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 664us/step - loss: 0.1160 - accuracy: 0.8351\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.1036 - accuracy: 0.8883\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.1033 - accuracy: 0.8564\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.1000 - accuracy: 0.8670\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.1074 - accuracy: 0.8404\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.1008 - accuracy: 0.8723\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.0954 - accuracy: 0.8723\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.0958 - accuracy: 0.8989\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 687us/step - loss: 0.0960 - accuracy: 0.9043\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.0919 - accuracy: 0.9202\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.0922 - accuracy: 0.8936\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.0895 - accuracy: 0.8989\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.0882 - accuracy: 0.9096\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.0875 - accuracy: 0.9149\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.0863 - accuracy: 0.9043\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0941 - accuracy: 0.9043\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.0835 - accuracy: 0.8989\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.0869 - accuracy: 0.8883\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0833 - accuracy: 0.9202\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.0789 - accuracy: 0.9255\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 712us/step - loss: 0.0775 - accuracy: 0.9202\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.0774 - accuracy: 0.9202\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.0760 - accuracy: 0.9255\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.0740 - accuracy: 0.9255\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 685us/step - loss: 0.0698 - accuracy: 0.9415\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 672us/step - loss: 0.0720 - accuracy: 0.9415\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 684us/step - loss: 0.0698 - accuracy: 0.9362\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.0729 - accuracy: 0.9043\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.0665 - accuracy: 0.9415\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.0694 - accuracy: 0.9202\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.0664 - accuracy: 0.9362\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0612 - accuracy: 0.9362\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.0664 - accuracy: 0.9309\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.0592 - accuracy: 0.9468\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0602 - accuracy: 0.9362\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.0620 - accuracy: 0.9415\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.0563 - accuracy: 0.9521\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 735us/step - loss: 0.0538 - accuracy: 0.9521\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.0539 - accuracy: 0.9521\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.0537 - accuracy: 0.9468\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.0505 - accuracy: 0.9574\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 697us/step - loss: 0.0510 - accuracy: 0.9415\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 689us/step - loss: 0.0474 - accuracy: 0.9681\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.0476 - accuracy: 0.9681\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.0463 - accuracy: 0.9521\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.0440 - accuracy: 0.9681\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0436 - accuracy: 0.9574\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.0429 - accuracy: 0.9734\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.0412 - accuracy: 0.9734\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0399 - accuracy: 0.9628\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 696us/step - loss: 0.0414 - accuracy: 0.9734\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.0392 - accuracy: 0.9734\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 691us/step - loss: 0.0362 - accuracy: 0.9734\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 675us/step - loss: 0.0374 - accuracy: 0.9734\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0371 - accuracy: 0.9840\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.0337 - accuracy: 0.9840\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.0328 - accuracy: 0.9787\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.0325 - accuracy: 0.9787\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.0328 - accuracy: 0.9787\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.0292 - accuracy: 0.9840\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.0319 - accuracy: 0.9840\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.0323 - accuracy: 0.9734\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.0298 - accuracy: 0.9734\n",
      "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x00000199FAFB5828> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 0.0946 - accuracy: 0.9500\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 692us/step - loss: 0.2483 - accuracy: 0.5372\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 635us/step - loss: 0.2414 - accuracy: 0.5638\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 681us/step - loss: 0.2369 - accuracy: 0.5798\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2301 - accuracy: 0.6383\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.2250 - accuracy: 0.6543\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.2197 - accuracy: 0.6596\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.2117 - accuracy: 0.7021\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.2041 - accuracy: 0.7021\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.1961 - accuracy: 0.7074\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.1884 - accuracy: 0.7713\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.1838 - accuracy: 0.7287\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 700us/step - loss: 0.1724 - accuracy: 0.8032\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.1683 - accuracy: 0.7766\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 666us/step - loss: 0.1619 - accuracy: 0.7819\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.1605 - accuracy: 0.7713\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.1507 - accuracy: 0.8191\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.1541 - accuracy: 0.7819\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 663us/step - loss: 0.1467 - accuracy: 0.8085\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.1402 - accuracy: 0.8245\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 689us/step - loss: 0.1395 - accuracy: 0.8032\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.1377 - accuracy: 0.7926\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.1309 - accuracy: 0.8085\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 1.00 - 0s 748us/step - loss: 0.1252 - accuracy: 0.8404\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.1258 - accuracy: 0.8191\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.1233 - accuracy: 0.8404\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1212 - accuracy: 0.8351\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.1205 - accuracy: 0.8404\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.1140 - accuracy: 0.8564\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.1169 - accuracy: 0.8511\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.1164 - accuracy: 0.8564\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 718us/step - loss: 0.1078 - accuracy: 0.8564\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.1071 - accuracy: 0.8617\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.1053 - accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.1032 - accuracy: 0.8830\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.1019 - accuracy: 0.8777\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.0999 - accuracy: 0.8670\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.0962 - accuracy: 0.8883\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 691us/step - loss: 0.0964 - accuracy: 0.8936\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 709us/step - loss: 0.0976 - accuracy: 0.8830\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.0922 - accuracy: 0.8936\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.0915 - accuracy: 0.8989\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 711us/step - loss: 0.0856 - accuracy: 0.9149\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.0899 - accuracy: 0.8883\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.0862 - accuracy: 0.9043\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.0862 - accuracy: 0.8830\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.0829 - accuracy: 0.8883\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.0819 - accuracy: 0.9043\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.0792 - accuracy: 0.9255\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 721us/step - loss: 0.0771 - accuracy: 0.9202\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.0792 - accuracy: 0.9043\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 669us/step - loss: 0.0751 - accuracy: 0.9149\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.0754 - accuracy: 0.9202\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 693us/step - loss: 0.0743 - accuracy: 0.8989\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 698us/step - loss: 0.0726 - accuracy: 0.9149\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 684us/step - loss: 0.0747 - accuracy: 0.9096\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 680us/step - loss: 0.0685 - accuracy: 0.9202\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 676us/step - loss: 0.0668 - accuracy: 0.9255\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 707us/step - loss: 0.0634 - accuracy: 0.9468\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 687us/step - loss: 0.0670 - accuracy: 0.9255\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0655 - accuracy: 0.9255\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 658us/step - loss: 0.0626 - accuracy: 0.9415\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 695us/step - loss: 0.0629 - accuracy: 0.9255\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 659us/step - loss: 0.0604 - accuracy: 0.9362\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 665us/step - loss: 0.0587 - accuracy: 0.9362\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 664us/step - loss: 0.0552 - accuracy: 0.9521\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 675us/step - loss: 0.0560 - accuracy: 0.9415\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.0533 - accuracy: 0.9521\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 657us/step - loss: 0.0507 - accuracy: 0.9521\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 703us/step - loss: 0.0622 - accuracy: 0.9043\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 682us/step - loss: 0.0573 - accuracy: 0.9468\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.0532 - accuracy: 0.9468\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 730us/step - loss: 0.0502 - accuracy: 0.9309\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.0484 - accuracy: 0.9521\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 664us/step - loss: 0.0466 - accuracy: 0.9521\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.0439 - accuracy: 0.9628\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 649us/step - loss: 0.0516 - accuracy: 0.9415\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 705us/step - loss: 0.0437 - accuracy: 0.9681\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 672us/step - loss: 0.0421 - accuracy: 0.9787\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.0430 - accuracy: 0.9628\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 703us/step - loss: 0.0465 - accuracy: 0.9521\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.0408 - accuracy: 0.9628\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 667us/step - loss: 0.0393 - accuracy: 0.9734\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 704us/step - loss: 0.0383 - accuracy: 0.9681\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 690us/step - loss: 0.0365 - accuracy: 0.9787\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 676us/step - loss: 0.0377 - accuracy: 0.9734\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.0360 - accuracy: 0.9787\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.0341 - accuracy: 0.9840\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 720us/step - loss: 0.0320 - accuracy: 0.9787\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.0330 - accuracy: 0.9787\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 640us/step - loss: 0.0301 - accuracy: 0.9840\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 713us/step - loss: 0.0316 - accuracy: 0.9787\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 723us/step - loss: 0.0306 - accuracy: 0.9894\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 727us/step - loss: 0.0330 - accuracy: 0.9681\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 683us/step - loss: 0.0280 - accuracy: 0.9840\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 708us/step - loss: 0.0286 - accuracy: 0.9840\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 686us/step - loss: 0.0313 - accuracy: 0.9734\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.0278 - accuracy: 0.9787\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 722us/step - loss: 0.0246 - accuracy: 0.9840\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 684us/step - loss: 0.0225 - accuracy: 0.9840\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 672us/step - loss: 0.0271 - accuracy: 0.9681\n",
      "WARNING:tensorflow:11 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000019980070558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.8500\n",
      "\n",
      "10 fold accuracy: ['0.7143', '0.8095', '0.7619', '0.9048', '0.8571', '0.7619', '0.8571', '0.7619', '0.9500', '0.8500']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder#원핫인코딩\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "dataset=df.values\n",
    "X=dataset[:,0:60].astype(float)\n",
    "Y_obj=dataset[:,60]\n",
    "\n",
    "#원핫인코딩\n",
    "#문자열을 숫자로 변환\n",
    "e=LabelEncoder()\n",
    "e.fit(Y_obj)\n",
    "Y=e.transform(Y_obj)\n",
    "\n",
    "#열개의 파일로 쪼갬\n",
    "n_fold=10\n",
    "skf=StratifiedKFold(n_splits=n_fold,shuffle=True,random_state=seed)\n",
    "\n",
    "accuracy=[]\n",
    "for train, test in skf.split(X,Y):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(24,input_dim=60,activation='relu'))\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    X_train=tf.convert_to_tensor(X[train],dtype=tf.float32)\n",
    "    Y_train=tf.convert_to_tensor(Y[train],dtype=tf.float32)\n",
    "    model.fit(X_train,Y_train,epochs=100,batch_size=5)\n",
    "    X_test=tf.convert_to_tensor(X[test],dtype=tf.float32)\n",
    "    Y_test=tf.convert_to_tensor(Y[test],dtype=tf.float32)\n",
    "    k_accuracy='%.4f'%(model.evaluate(X_test,Y_test)[1])\n",
    "    accuracy.append(k_accuracy)\n",
    "    \n",
    "print('\\n%.f fold accuracy:'%n_fold,accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wine.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre=pd.read_csv('wine.csv',header=None)\n",
    "df=df_pre.sample(frac=1)\n",
    "#frac=1은 원본 데이터의 100%를 불러오라는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.24</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.053</td>\n",
       "      <td>20.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.99373</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.047</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.99164</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.54</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.50</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0.049</td>\n",
       "      <td>56.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.99940</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.66</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.074</td>\n",
       "      <td>25.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99370</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.034</td>\n",
       "      <td>29.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99170</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3      4     5      6        7     8     9     10  \\\n",
       "5316  6.3  0.18  0.24   3.4  0.053  20.0  119.0  0.99373  3.11  0.52   9.2   \n",
       "5210  6.8  0.14  0.18   1.4  0.047  30.0   90.0  0.99164  3.27  0.54  11.2   \n",
       "3518  7.3  0.22  0.50  13.7  0.049  56.0  189.0  0.99940  3.24  0.66   9.0   \n",
       "1622  7.6  0.67  0.14   1.5  0.074  25.0  168.0  0.99370  3.05  0.51   9.3   \n",
       "2443  7.3  0.21  0.29   1.6  0.034  29.0  118.0  0.99170  3.30  0.50  11.0   \n",
       "\n",
       "      11  12  \n",
       "5316   6   0  \n",
       "5210   6   0  \n",
       "3518   6   0  \n",
       "1622   5   0  \n",
       "2443   8   0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6497 entries, 5316 to 2732\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       6497 non-null   float64\n",
      " 1   1       6497 non-null   float64\n",
      " 2   2       6497 non-null   float64\n",
      " 3   3       6497 non-null   float64\n",
      " 4   4       6497 non-null   float64\n",
      " 5   5       6497 non-null   float64\n",
      " 6   6       6497 non-null   float64\n",
      " 7   7       6497 non-null   float64\n",
      " 8   8       6497 non-null   float64\n",
      " 9   9       6497 non-null   float64\n",
      " 10  10      6497 non-null   float64\n",
      " 11  11      6497 non-null   int64  \n",
      " 12  12      6497 non-null   int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 710.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:976 __call__\n        self.name)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer sequential_21 is incompatible with the layer: expected axis -1 of input shape to have value 4 but received input with shape [None, 12]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-2062749fdc21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n Accuracy: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 697\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:976 __call__\n        self.name)\n    c:\\users\\user\\anaconda3\\envs\\jiwoo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer sequential_21 is incompatible with the layer: expected axis -1 of input shape to have value 4 but received input with shape [None, 12]\n"
     ]
    }
   ],
   "source": [
    "#데이터분류\n",
    "dataset=df.values\n",
    "X=dataset[:,0:12].astype(float)\n",
    "Y_obj=dataset[:,12]\n",
    "\n",
    "e=LabelEncoder()\n",
    "e.fit(Y_obj)\n",
    "Y=e.transform(Y_obj)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(30,input_dim=4,activation='relu'))\n",
    "model.add(Dense(12,activation='relu'))\n",
    "model.add(Dense(8,activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X,Y_obj,epochs=200,batch_size=5)\n",
    "\n",
    "print(\"\\n Accuracy: %.4f\" % model.evaluate(X,Y_obj)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000199F9C680D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29722, saving model to ./model\\01-0.2972.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.29722 to 0.25425, saving model to ./model\\02-0.2543.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25425 to 0.23356, saving model to ./model\\03-0.2336.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.23356 to 0.21444, saving model to ./model\\04-0.2144.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.21444 to 0.19984, saving model to ./model\\05-0.1998.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.19984 to 0.19539, saving model to ./model\\06-0.1954.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.19539 to 0.19016, saving model to ./model\\07-0.1902.hdf5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.19016\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.19016 to 0.18586, saving model to ./model\\09-0.1859.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.18586 to 0.18406, saving model to ./model\\10-0.1841.hdf5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.18406\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.18406 to 0.17748, saving model to ./model\\12-0.1775.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.17748 to 0.17532, saving model to ./model\\13-0.1753.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.17532 to 0.17184, saving model to ./model\\14-0.1718.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.17184 to 0.16910, saving model to ./model\\15-0.1691.hdf5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.16910\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.16910 to 0.16181, saving model to ./model\\17-0.1618.hdf5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.16181\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.16181 to 0.15618, saving model to ./model\\19-0.1562.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.15618 to 0.15234, saving model to ./model\\20-0.1523.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.15234 to 0.14853, saving model to ./model\\21-0.1485.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.14853 to 0.14607, saving model to ./model\\22-0.1461.hdf5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.14607\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.14607 to 0.14017, saving model to ./model\\24-0.1402.hdf5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.14017 to 0.13896, saving model to ./model\\25-0.1390.hdf5\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.13896 to 0.13648, saving model to ./model\\26-0.1365.hdf5\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.13648 to 0.13404, saving model to ./model\\27-0.1340.hdf5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.13404\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.13404 to 0.12704, saving model to ./model\\29-0.1270.hdf5\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.12704 to 0.12473, saving model to ./model\\30-0.1247.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.12473 to 0.12384, saving model to ./model\\31-0.1238.hdf5\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.12384 to 0.12259, saving model to ./model\\32-0.1226.hdf5\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.12259 to 0.11661, saving model to ./model\\33-0.1166.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.11661 to 0.11613, saving model to ./model\\34-0.1161.hdf5\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.11613 to 0.11273, saving model to ./model\\35-0.1127.hdf5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11273\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11273\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.11273 to 0.10735, saving model to ./model\\38-0.1074.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.10735 to 0.10549, saving model to ./model\\39-0.1055.hdf5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.10549\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.10549\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.10549 to 0.09682, saving model to ./model\\42-0.0968.hdf5\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09682 to 0.09665, saving model to ./model\\43-0.0967.hdf5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09665\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09665 to 0.09266, saving model to ./model\\45-0.0927.hdf5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09266 to 0.09130, saving model to ./model\\46-0.0913.hdf5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09130\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09130\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09130 to 0.08785, saving model to ./model\\49-0.0879.hdf5\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.08785 to 0.08720, saving model to ./model\\50-0.0872.hdf5\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.08720 to 0.08371, saving model to ./model\\51-0.0837.hdf5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.08371\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.08371\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.08371\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.08371 to 0.07985, saving model to ./model\\55-0.0798.hdf5\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.07985 to 0.07835, saving model to ./model\\56-0.0784.hdf5\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.07835 to 0.07805, saving model to ./model\\57-0.0781.hdf5\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.07805\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.07805 to 0.07646, saving model to ./model\\59-0.0765.hdf5\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.07646 to 0.07473, saving model to ./model\\60-0.0747.hdf5\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.07473\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.07473\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.07473\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.07473 to 0.07291, saving model to ./model\\64-0.0729.hdf5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.07291\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.07291 to 0.07016, saving model to ./model\\66-0.0702.hdf5\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.07016\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.07016 to 0.06992, saving model to ./model\\68-0.0699.hdf5\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.06992\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.06992\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.06992 to 0.06726, saving model to ./model\\71-0.0673.hdf5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.06726\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.06726\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.06726 to 0.06652, saving model to ./model\\74-0.0665.hdf5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.06652\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.06652\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.06652 to 0.06574, saving model to ./model\\77-0.0657.hdf5\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.06574 to 0.06550, saving model to ./model\\78-0.0655.hdf5\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.06550\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.06550\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.06550\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.06550 to 0.06544, saving model to ./model\\82-0.0654.hdf5\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.06544 to 0.06391, saving model to ./model\\83-0.0639.hdf5\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.06391 to 0.06286, saving model to ./model\\84-0.0629.hdf5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.06286\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.06286\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.06286\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.06286 to 0.06137, saving model to ./model\\88-0.0614.hdf5\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.06137\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.06137\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.06137\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.06137\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.06137 to 0.06019, saving model to ./model\\93-0.0602.hdf5\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.06019\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.06019\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.06019 to 0.06018, saving model to ./model\\96-0.0602.hdf5\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.06018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00098: val_loss improved from 0.06018 to 0.05950, saving model to ./model\\98-0.0595.hdf5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.05950\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.05950\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.05950 to 0.05810, saving model to ./model\\101-0.0581.hdf5\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.05810\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.05810\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.05810 to 0.05765, saving model to ./model\\104-0.0577.hdf5\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.05765 to 0.05686, saving model to ./model\\105-0.0569.hdf5\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.05686\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.05686\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.05686\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.05686\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.05686\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.05686 to 0.05665, saving model to ./model\\111-0.0567.hdf5\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.05665\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.05665 to 0.05583, saving model to ./model\\113-0.0558.hdf5\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.05583\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.05583\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.05583\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.05583 to 0.05531, saving model to ./model\\117-0.0553.hdf5\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.05531\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.05531\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.05531\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.05531\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.05531\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.05531\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.05531\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.05531 to 0.05483, saving model to ./model\\125-0.0548.hdf5\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.05483\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.05483\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.05483 to 0.05457, saving model to ./model\\128-0.0546.hdf5\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.05457\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.05457 to 0.05359, saving model to ./model\\130-0.0536.hdf5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.05359\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.05359\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.05359\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.05359\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.05359 to 0.05315, saving model to ./model\\135-0.0532.hdf5\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.05315\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.05315\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.05315\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.05315\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.05315\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.05315 to 0.05294, saving model to ./model\\141-0.0529.hdf5\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.05294\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.05294\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.05294\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.05294\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.05294\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.05294 to 0.05247, saving model to ./model\\147-0.0525.hdf5\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.05247\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.05247\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.05247\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.05247\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.05247\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.05247\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.05247\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.05247\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.05247 to 0.05240, saving model to ./model\\156-0.0524.hdf5\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.05240\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.05240\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.05240\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.05240\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.05240\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.05240 to 0.05213, saving model to ./model\\162-0.0521.hdf5\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.05213\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.05213\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.05213\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.05213 to 0.05133, saving model to ./model\\166-0.0513.hdf5\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.05133\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.05133 to 0.05058, saving model to ./model\\197-0.0506.hdf5\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.05058\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.05058\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.05058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x199fff97048>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "dataset=df.values\n",
    "X=dataset[:,0:12].astype(float)\n",
    "Y=dataset[:,12]\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(30,input_dim=12,activation='relu'))\n",
    "model.add(Dense(12,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "MODEL_DIR='./model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "    \n",
    "modelpath='./model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "#검증의 오차값을 표시(verbose는 0이면 표시 안함)\n",
    "checkpointer=ModelCheckpoint(filepath=modelpath, monitor='val_loss',verbose=1,\n",
    "                            save_best_only=True)\n",
    "#checkpointer라는 변수를 만들어 이곳에 모니터할 값을 지정\n",
    "\n",
    "model.fit(X,Y,validation_split=0.2,epochs=200,batch_size=200,verbose=0,\n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7844 - accuracy: 0.7596 - val_loss: 0.5725 - val_accuracy: 0.7795\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5981 - accuracy: 0.7642 - val_loss: 0.4143 - val_accuracy: 0.7826\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4410 - accuracy: 0.7718 - val_loss: 0.3250 - val_accuracy: 0.7950\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3739 - accuracy: 0.7734 - val_loss: 0.3308 - val_accuracy: 0.8416\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3827 - accuracy: 0.8101 - val_loss: 0.3423 - val_accuracy: 0.9068\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3818 - accuracy: 0.8499 - val_loss: 0.3260 - val_accuracy: 0.9193\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3588 - accuracy: 0.8698 - val_loss: 0.3058 - val_accuracy: 0.9130\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3363 - accuracy: 0.8760 - val_loss: 0.2917 - val_accuracy: 0.9099\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3218 - accuracy: 0.8821 - val_loss: 0.2868 - val_accuracy: 0.9037\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3148 - accuracy: 0.8806 - val_loss: 0.2863 - val_accuracy: 0.8944\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3131 - accuracy: 0.8806 - val_loss: 0.2864 - val_accuracy: 0.8913\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3126 - accuracy: 0.8867 - val_loss: 0.2858 - val_accuracy: 0.8851\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3110 - accuracy: 0.8928 - val_loss: 0.2836 - val_accuracy: 0.8913\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3086 - accuracy: 0.8913 - val_loss: 0.2797 - val_accuracy: 0.8944\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3041 - accuracy: 0.8943 - val_loss: 0.2745 - val_accuracy: 0.9037\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2990 - accuracy: 0.8989 - val_loss: 0.2690 - val_accuracy: 0.9037\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2934 - accuracy: 0.8989 - val_loss: 0.2644 - val_accuracy: 0.9037\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2890 - accuracy: 0.8989 - val_loss: 0.2609 - val_accuracy: 0.9068\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2855 - accuracy: 0.9051 - val_loss: 0.2585 - val_accuracy: 0.9130\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2818 - accuracy: 0.9066 - val_loss: 0.2575 - val_accuracy: 0.9161\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2801 - accuracy: 0.9081 - val_loss: 0.2581 - val_accuracy: 0.9130\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2787 - accuracy: 0.9081 - val_loss: 0.2586 - val_accuracy: 0.9099\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2776 - accuracy: 0.9142 - val_loss: 0.2572 - val_accuracy: 0.9130\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2747 - accuracy: 0.9142 - val_loss: 0.2536 - val_accuracy: 0.9130\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2701 - accuracy: 0.9112 - val_loss: 0.2503 - val_accuracy: 0.9130\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2668 - accuracy: 0.9066 - val_loss: 0.2485 - val_accuracy: 0.9161\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2639 - accuracy: 0.9066 - val_loss: 0.2482 - val_accuracy: 0.9130\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2632 - accuracy: 0.9051 - val_loss: 0.2482 - val_accuracy: 0.9130\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2623 - accuracy: 0.9035 - val_loss: 0.2475 - val_accuracy: 0.9130\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2603 - accuracy: 0.9035 - val_loss: 0.2453 - val_accuracy: 0.9161\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2578 - accuracy: 0.9035 - val_loss: 0.2430 - val_accuracy: 0.9130\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2552 - accuracy: 0.9081 - val_loss: 0.2412 - val_accuracy: 0.9130\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2527 - accuracy: 0.9112 - val_loss: 0.2398 - val_accuracy: 0.9130\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2513 - accuracy: 0.9127 - val_loss: 0.2386 - val_accuracy: 0.9161\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2495 - accuracy: 0.9142 - val_loss: 0.2373 - val_accuracy: 0.9161\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2478 - accuracy: 0.9158 - val_loss: 0.2360 - val_accuracy: 0.9193\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2459 - accuracy: 0.9204 - val_loss: 0.2349 - val_accuracy: 0.9224\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2441 - accuracy: 0.9204 - val_loss: 0.2336 - val_accuracy: 0.9255\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2425 - accuracy: 0.9204 - val_loss: 0.2319 - val_accuracy: 0.9255\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2407 - accuracy: 0.9204 - val_loss: 0.2302 - val_accuracy: 0.9255\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2383 - accuracy: 0.9204 - val_loss: 0.2287 - val_accuracy: 0.9255\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2362 - accuracy: 0.9204 - val_loss: 0.2273 - val_accuracy: 0.9255\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2343 - accuracy: 0.9219 - val_loss: 0.2262 - val_accuracy: 0.9255\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2323 - accuracy: 0.9219 - val_loss: 0.2251 - val_accuracy: 0.9255\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2304 - accuracy: 0.9219 - val_loss: 0.2239 - val_accuracy: 0.9255\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2284 - accuracy: 0.9219 - val_loss: 0.2227 - val_accuracy: 0.9255\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2264 - accuracy: 0.9219 - val_loss: 0.2216 - val_accuracy: 0.9255\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2245 - accuracy: 0.9234 - val_loss: 0.2206 - val_accuracy: 0.9317\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2226 - accuracy: 0.9250 - val_loss: 0.2194 - val_accuracy: 0.9317\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2210 - accuracy: 0.9250 - val_loss: 0.2177 - val_accuracy: 0.9286\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2193 - accuracy: 0.9250 - val_loss: 0.2160 - val_accuracy: 0.9317\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2176 - accuracy: 0.9250 - val_loss: 0.2143 - val_accuracy: 0.9348\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2165 - accuracy: 0.9234 - val_loss: 0.2129 - val_accuracy: 0.9348\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2150 - accuracy: 0.9250 - val_loss: 0.2113 - val_accuracy: 0.9348\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2136 - accuracy: 0.9265 - val_loss: 0.2102 - val_accuracy: 0.9348\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2129 - accuracy: 0.9265 - val_loss: 0.2093 - val_accuracy: 0.9379\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2115 - accuracy: 0.9280 - val_loss: 0.2083 - val_accuracy: 0.9379\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2105 - accuracy: 0.9265 - val_loss: 0.2075 - val_accuracy: 0.9410\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2096 - accuracy: 0.9265 - val_loss: 0.2066 - val_accuracy: 0.9410\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2088 - accuracy: 0.9265 - val_loss: 0.2058 - val_accuracy: 0.9410\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2081 - accuracy: 0.9265 - val_loss: 0.2051 - val_accuracy: 0.9410\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2076 - accuracy: 0.9280 - val_loss: 0.2043 - val_accuracy: 0.9410\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2070 - accuracy: 0.9280 - val_loss: 0.2034 - val_accuracy: 0.9410\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2064 - accuracy: 0.9296 - val_loss: 0.2025 - val_accuracy: 0.9410\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2058 - accuracy: 0.9296 - val_loss: 0.2017 - val_accuracy: 0.9410\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2051 - accuracy: 0.9296 - val_loss: 0.2009 - val_accuracy: 0.9410\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2044 - accuracy: 0.9296 - val_loss: 0.2001 - val_accuracy: 0.9410\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2037 - accuracy: 0.9296 - val_loss: 0.1993 - val_accuracy: 0.9410\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2032 - accuracy: 0.9296 - val_loss: 0.1985 - val_accuracy: 0.9410\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2026 - accuracy: 0.9296 - val_loss: 0.1978 - val_accuracy: 0.9441\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2022 - accuracy: 0.9296 - val_loss: 0.1973 - val_accuracy: 0.9441\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2017 - accuracy: 0.9280 - val_loss: 0.1968 - val_accuracy: 0.9472\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2013 - accuracy: 0.9280 - val_loss: 0.1965 - val_accuracy: 0.9472\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2008 - accuracy: 0.9296 - val_loss: 0.1963 - val_accuracy: 0.9472\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2006 - accuracy: 0.9296 - val_loss: 0.1960 - val_accuracy: 0.9472\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2000 - accuracy: 0.9296 - val_loss: 0.1955 - val_accuracy: 0.9472\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1992 - accuracy: 0.9296 - val_loss: 0.1952 - val_accuracy: 0.9472\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1989 - accuracy: 0.9311 - val_loss: 0.1957 - val_accuracy: 0.9503\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1994 - accuracy: 0.9296 - val_loss: 0.1959 - val_accuracy: 0.9472\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1990 - accuracy: 0.9296 - val_loss: 0.1949 - val_accuracy: 0.9472\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1984 - accuracy: 0.9311 - val_loss: 0.1942 - val_accuracy: 0.9472\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1969 - accuracy: 0.9311 - val_loss: 0.1940 - val_accuracy: 0.9472\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1964 - accuracy: 0.9326 - val_loss: 0.1942 - val_accuracy: 0.9472\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1962 - accuracy: 0.9311 - val_loss: 0.1948 - val_accuracy: 0.9441\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1965 - accuracy: 0.9311 - val_loss: 0.1955 - val_accuracy: 0.9441\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1968 - accuracy: 0.9311 - val_loss: 0.1955 - val_accuracy: 0.9441\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1966 - accuracy: 0.9311 - val_loss: 0.1944 - val_accuracy: 0.9441\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1958 - accuracy: 0.9311 - val_loss: 0.1925 - val_accuracy: 0.9472\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1944 - accuracy: 0.9326 - val_loss: 0.1915 - val_accuracy: 0.9472\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1940 - accuracy: 0.9326 - val_loss: 0.1910 - val_accuracy: 0.9472\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1934 - accuracy: 0.9326 - val_loss: 0.1908 - val_accuracy: 0.9472\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1931 - accuracy: 0.9326 - val_loss: 0.1906 - val_accuracy: 0.9472\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1927 - accuracy: 0.9326 - val_loss: 0.1904 - val_accuracy: 0.9472\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1923 - accuracy: 0.9311 - val_loss: 0.1903 - val_accuracy: 0.9472\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1919 - accuracy: 0.9326 - val_loss: 0.1903 - val_accuracy: 0.9472\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1916 - accuracy: 0.9326 - val_loss: 0.1902 - val_accuracy: 0.9472\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1913 - accuracy: 0.9326 - val_loss: 0.1901 - val_accuracy: 0.9472\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1910 - accuracy: 0.9326 - val_loss: 0.1900 - val_accuracy: 0.9472\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1907 - accuracy: 0.9342 - val_loss: 0.1897 - val_accuracy: 0.9503\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1906 - accuracy: 0.9326 - val_loss: 0.1895 - val_accuracy: 0.9503\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1905 - accuracy: 0.9326 - val_loss: 0.1894 - val_accuracy: 0.9503\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1903 - accuracy: 0.9326 - val_loss: 0.1890 - val_accuracy: 0.9503\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1896 - accuracy: 0.9326 - val_loss: 0.1888 - val_accuracy: 0.9503\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1890 - accuracy: 0.9357 - val_loss: 0.1889 - val_accuracy: 0.9472\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1889 - accuracy: 0.9342 - val_loss: 0.1886 - val_accuracy: 0.9472\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1890 - accuracy: 0.9342 - val_loss: 0.1883 - val_accuracy: 0.9472\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1889 - accuracy: 0.9342 - val_loss: 0.1883 - val_accuracy: 0.9472\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1887 - accuracy: 0.9342 - val_loss: 0.1872 - val_accuracy: 0.9472\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1877 - accuracy: 0.9342 - val_loss: 0.1861 - val_accuracy: 0.9472\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1869 - accuracy: 0.9342 - val_loss: 0.1849 - val_accuracy: 0.9472\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1864 - accuracy: 0.9357 - val_loss: 0.1844 - val_accuracy: 0.9503\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1864 - accuracy: 0.9342 - val_loss: 0.1842 - val_accuracy: 0.9503\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1864 - accuracy: 0.9342 - val_loss: 0.1839 - val_accuracy: 0.9503\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1861 - accuracy: 0.9342 - val_loss: 0.1837 - val_accuracy: 0.9503\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1855 - accuracy: 0.9357 - val_loss: 0.1837 - val_accuracy: 0.9503\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1849 - accuracy: 0.9357 - val_loss: 0.1834 - val_accuracy: 0.9472\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1845 - accuracy: 0.9357 - val_loss: 0.1830 - val_accuracy: 0.9472\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1844 - accuracy: 0.9357 - val_loss: 0.1829 - val_accuracy: 0.9472\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1840 - accuracy: 0.9357 - val_loss: 0.1835 - val_accuracy: 0.9441\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1843 - accuracy: 0.9342 - val_loss: 0.1838 - val_accuracy: 0.9441\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1842 - accuracy: 0.9342 - val_loss: 0.1832 - val_accuracy: 0.9441\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1838 - accuracy: 0.9342 - val_loss: 0.1823 - val_accuracy: 0.9441\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1828 - accuracy: 0.9357 - val_loss: 0.1818 - val_accuracy: 0.9503\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1824 - accuracy: 0.9357 - val_loss: 0.1815 - val_accuracy: 0.9503\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1819 - accuracy: 0.9357 - val_loss: 0.1812 - val_accuracy: 0.9503\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1817 - accuracy: 0.9357 - val_loss: 0.1807 - val_accuracy: 0.9503\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1813 - accuracy: 0.9357 - val_loss: 0.1803 - val_accuracy: 0.9503\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1811 - accuracy: 0.9357 - val_loss: 0.1798 - val_accuracy: 0.9503\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1808 - accuracy: 0.9357 - val_loss: 0.1798 - val_accuracy: 0.9503\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1803 - accuracy: 0.9357 - val_loss: 0.1807 - val_accuracy: 0.9441\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1805 - accuracy: 0.9342 - val_loss: 0.1813 - val_accuracy: 0.9441\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1808 - accuracy: 0.9342 - val_loss: 0.1814 - val_accuracy: 0.9441\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1808 - accuracy: 0.9342 - val_loss: 0.1806 - val_accuracy: 0.9441\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1799 - accuracy: 0.9342 - val_loss: 0.1786 - val_accuracy: 0.9472\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1785 - accuracy: 0.9357 - val_loss: 0.1774 - val_accuracy: 0.9503\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1790 - accuracy: 0.9372 - val_loss: 0.1778 - val_accuracy: 0.9472\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1801 - accuracy: 0.9387 - val_loss: 0.1773 - val_accuracy: 0.9472\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1788 - accuracy: 0.9387 - val_loss: 0.1769 - val_accuracy: 0.9503\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1788 - accuracy: 0.9342 - val_loss: 0.1791 - val_accuracy: 0.9441\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1789 - accuracy: 0.9342 - val_loss: 0.1795 - val_accuracy: 0.9441\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1785 - accuracy: 0.9342 - val_loss: 0.1778 - val_accuracy: 0.9472\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1776 - accuracy: 0.9342 - val_loss: 0.1760 - val_accuracy: 0.9472\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1765 - accuracy: 0.9357 - val_loss: 0.1755 - val_accuracy: 0.9472\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1763 - accuracy: 0.9357 - val_loss: 0.1753 - val_accuracy: 0.9472\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.9357 - val_loss: 0.1753 - val_accuracy: 0.9472\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1757 - accuracy: 0.9357 - val_loss: 0.1757 - val_accuracy: 0.9472\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1755 - accuracy: 0.9357 - val_loss: 0.1766 - val_accuracy: 0.9441\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.9342 - val_loss: 0.1768 - val_accuracy: 0.9441\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.9342 - val_loss: 0.1751 - val_accuracy: 0.9472\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1749 - accuracy: 0.9342 - val_loss: 0.1732 - val_accuracy: 0.9472\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1739 - accuracy: 0.9357 - val_loss: 0.1719 - val_accuracy: 0.9472\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1740 - accuracy: 0.9357 - val_loss: 0.1725 - val_accuracy: 0.9472\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1755 - accuracy: 0.9372 - val_loss: 0.1726 - val_accuracy: 0.9472\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1746 - accuracy: 0.9387 - val_loss: 0.1723 - val_accuracy: 0.9441\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1731 - accuracy: 0.9357 - val_loss: 0.1748 - val_accuracy: 0.9472\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1734 - accuracy: 0.9326 - val_loss: 0.1765 - val_accuracy: 0.9472\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1743 - accuracy: 0.9326 - val_loss: 0.1757 - val_accuracy: 0.9472\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1737 - accuracy: 0.9326 - val_loss: 0.1729 - val_accuracy: 0.9472\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1719 - accuracy: 0.9326 - val_loss: 0.1708 - val_accuracy: 0.9472\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1716 - accuracy: 0.9357 - val_loss: 0.1704 - val_accuracy: 0.9441\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1721 - accuracy: 0.9357 - val_loss: 0.1708 - val_accuracy: 0.9441\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1730 - accuracy: 0.9357 - val_loss: 0.1705 - val_accuracy: 0.9441\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1719 - accuracy: 0.9342 - val_loss: 0.1707 - val_accuracy: 0.9472\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1710 - accuracy: 0.9342 - val_loss: 0.1716 - val_accuracy: 0.9472\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1703 - accuracy: 0.9326 - val_loss: 0.1707 - val_accuracy: 0.9472\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1697 - accuracy: 0.9326 - val_loss: 0.1698 - val_accuracy: 0.9472\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1690 - accuracy: 0.9342 - val_loss: 0.1689 - val_accuracy: 0.9441\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1695 - accuracy: 0.9357 - val_loss: 0.1685 - val_accuracy: 0.9441\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1692 - accuracy: 0.9342 - val_loss: 0.1684 - val_accuracy: 0.9441\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1685 - accuracy: 0.9342 - val_loss: 0.1681 - val_accuracy: 0.9441\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1681 - accuracy: 0.9342 - val_loss: 0.1675 - val_accuracy: 0.9441\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1682 - accuracy: 0.9372 - val_loss: 0.1669 - val_accuracy: 0.9441\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1676 - accuracy: 0.9342 - val_loss: 0.1665 - val_accuracy: 0.9441\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1667 - accuracy: 0.9342 - val_loss: 0.1674 - val_accuracy: 0.9472\n",
      "Epoch 175/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1676 - accuracy: 0.9342 - val_loss: 0.1679 - val_accuracy: 0.9472\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1672 - accuracy: 0.9342 - val_loss: 0.1658 - val_accuracy: 0.9472\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1666 - accuracy: 0.9342 - val_loss: 0.1649 - val_accuracy: 0.9441\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1662 - accuracy: 0.9372 - val_loss: 0.1650 - val_accuracy: 0.9441\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1653 - accuracy: 0.9357 - val_loss: 0.1664 - val_accuracy: 0.9472\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1651 - accuracy: 0.9342 - val_loss: 0.1677 - val_accuracy: 0.9472\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1653 - accuracy: 0.9342 - val_loss: 0.1676 - val_accuracy: 0.9472\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1652 - accuracy: 0.9342 - val_loss: 0.1669 - val_accuracy: 0.9472\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1644 - accuracy: 0.9342 - val_loss: 0.1668 - val_accuracy: 0.9472\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1640 - accuracy: 0.9342 - val_loss: 0.1666 - val_accuracy: 0.9472\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1637 - accuracy: 0.9357 - val_loss: 0.1666 - val_accuracy: 0.9441\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1638 - accuracy: 0.9372 - val_loss: 0.1677 - val_accuracy: 0.9472\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1631 - accuracy: 0.9342 - val_loss: 0.1710 - val_accuracy: 0.9472\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1647 - accuracy: 0.9357 - val_loss: 0.1705 - val_accuracy: 0.9472\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1639 - accuracy: 0.9357 - val_loss: 0.1656 - val_accuracy: 0.9472\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1617 - accuracy: 0.9357 - val_loss: 0.1636 - val_accuracy: 0.9441\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1638 - accuracy: 0.9387 - val_loss: 0.1639 - val_accuracy: 0.9410\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1655 - accuracy: 0.9357 - val_loss: 0.1621 - val_accuracy: 0.9441\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1615 - accuracy: 0.9387 - val_loss: 0.1637 - val_accuracy: 0.9472\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1622 - accuracy: 0.9342 - val_loss: 0.1687 - val_accuracy: 0.9472\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1644 - accuracy: 0.9372 - val_loss: 0.1655 - val_accuracy: 0.9472\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1623 - accuracy: 0.9372 - val_loss: 0.1607 - val_accuracy: 0.9441\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1601 - accuracy: 0.9372 - val_loss: 0.1602 - val_accuracy: 0.9441\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1606 - accuracy: 0.9387 - val_loss: 0.1603 - val_accuracy: 0.9441\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1592 - accuracy: 0.9372 - val_loss: 0.1622 - val_accuracy: 0.9472\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1592 - accuracy: 0.9342 - val_loss: 0.1664 - val_accuracy: 0.9472\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1612 - accuracy: 0.9372 - val_loss: 0.1666 - val_accuracy: 0.9472\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1604 - accuracy: 0.9372 - val_loss: 0.1628 - val_accuracy: 0.9472\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1583 - accuracy: 0.9357 - val_loss: 0.1606 - val_accuracy: 0.9441\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1582 - accuracy: 0.9357 - val_loss: 0.1607 - val_accuracy: 0.9441\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1598 - accuracy: 0.9387 - val_loss: 0.1605 - val_accuracy: 0.9441\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1593 - accuracy: 0.9372 - val_loss: 0.1595 - val_accuracy: 0.9441\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1568 - accuracy: 0.9387 - val_loss: 0.1611 - val_accuracy: 0.9472\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1564 - accuracy: 0.9342 - val_loss: 0.1632 - val_accuracy: 0.9472\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1577 - accuracy: 0.9372 - val_loss: 0.1631 - val_accuracy: 0.9472\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1583 - accuracy: 0.9372 - val_loss: 0.1609 - val_accuracy: 0.9472\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1570 - accuracy: 0.9372 - val_loss: 0.1584 - val_accuracy: 0.9472\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1559 - accuracy: 0.9357 - val_loss: 0.1550 - val_accuracy: 0.9441\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1550 - accuracy: 0.9357 - val_loss: 0.1538 - val_accuracy: 0.9441\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1558 - accuracy: 0.9387 - val_loss: 0.1536 - val_accuracy: 0.9441\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1554 - accuracy: 0.9357 - val_loss: 0.1542 - val_accuracy: 0.9410\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1566 - accuracy: 0.9372 - val_loss: 0.1539 - val_accuracy: 0.9441\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1548 - accuracy: 0.9357 - val_loss: 0.1559 - val_accuracy: 0.9441\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1532 - accuracy: 0.9357 - val_loss: 0.1575 - val_accuracy: 0.9472\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1538 - accuracy: 0.9357 - val_loss: 0.1578 - val_accuracy: 0.9472\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1537 - accuracy: 0.9372 - val_loss: 0.1571 - val_accuracy: 0.9472\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1530 - accuracy: 0.9357 - val_loss: 0.1559 - val_accuracy: 0.9441\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1523 - accuracy: 0.9372 - val_loss: 0.1551 - val_accuracy: 0.9441\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1534 - accuracy: 0.9387 - val_loss: 0.1548 - val_accuracy: 0.9441\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.94 - 0s 7ms/step - loss: 0.1521 - accuracy: 0.9387 - val_loss: 0.1557 - val_accuracy: 0.9441\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1508 - accuracy: 0.9342 - val_loss: 0.1580 - val_accuracy: 0.9472\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1515 - accuracy: 0.9372 - val_loss: 0.1585 - val_accuracy: 0.9472\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1515 - accuracy: 0.9372 - val_loss: 0.1568 - val_accuracy: 0.9441\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1500 - accuracy: 0.9372 - val_loss: 0.1548 - val_accuracy: 0.9441\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1496 - accuracy: 0.9387 - val_loss: 0.1539 - val_accuracy: 0.9441\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1511 - accuracy: 0.9387 - val_loss: 0.1526 - val_accuracy: 0.9441\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1492 - accuracy: 0.9387 - val_loss: 0.1534 - val_accuracy: 0.9441\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1487 - accuracy: 0.9372 - val_loss: 0.1559 - val_accuracy: 0.9472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1508 - accuracy: 0.9372 - val_loss: 0.1558 - val_accuracy: 0.9472\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1508 - accuracy: 0.9372 - val_loss: 0.1519 - val_accuracy: 0.9472\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1476 - accuracy: 0.9372 - val_loss: 0.1482 - val_accuracy: 0.9410\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1478 - accuracy: 0.9357 - val_loss: 0.1498 - val_accuracy: 0.9441\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1519 - accuracy: 0.9372 - val_loss: 0.1479 - val_accuracy: 0.9441\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1473 - accuracy: 0.9387 - val_loss: 0.1511 - val_accuracy: 0.9472\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1468 - accuracy: 0.9372 - val_loss: 0.1575 - val_accuracy: 0.9472\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1514 - accuracy: 0.9372 - val_loss: 0.1570 - val_accuracy: 0.9503\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1500 - accuracy: 0.9372 - val_loss: 0.1502 - val_accuracy: 0.9441\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1455 - accuracy: 0.9403 - val_loss: 0.1477 - val_accuracy: 0.9441\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1474 - accuracy: 0.9387 - val_loss: 0.1473 - val_accuracy: 0.9441\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1460 - accuracy: 0.9403 - val_loss: 0.1482 - val_accuracy: 0.9441\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1442 - accuracy: 0.9387 - val_loss: 0.1518 - val_accuracy: 0.9503\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1465 - accuracy: 0.9372 - val_loss: 0.1497 - val_accuracy: 0.9503\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1448 - accuracy: 0.9387 - val_loss: 0.1450 - val_accuracy: 0.9441\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1434 - accuracy: 0.9387 - val_loss: 0.1439 - val_accuracy: 0.9410\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1429 - accuracy: 0.9418 - val_loss: 0.1440 - val_accuracy: 0.9441\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1419 - accuracy: 0.9387 - val_loss: 0.1447 - val_accuracy: 0.9441\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1416 - accuracy: 0.9403 - val_loss: 0.1443 - val_accuracy: 0.9441\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1412 - accuracy: 0.9403 - val_loss: 0.1423 - val_accuracy: 0.9441\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1402 - accuracy: 0.9418 - val_loss: 0.1423 - val_accuracy: 0.9441\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1433 - accuracy: 0.9372 - val_loss: 0.1419 - val_accuracy: 0.9410\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1407 - accuracy: 0.9418 - val_loss: 0.1441 - val_accuracy: 0.9472\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1400 - accuracy: 0.9403 - val_loss: 0.1451 - val_accuracy: 0.9472\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1400 - accuracy: 0.9403 - val_loss: 0.1418 - val_accuracy: 0.9472\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1380 - accuracy: 0.9418 - val_loss: 0.1400 - val_accuracy: 0.9441\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1395 - accuracy: 0.9418 - val_loss: 0.1390 - val_accuracy: 0.9441\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1389 - accuracy: 0.9387 - val_loss: 0.1386 - val_accuracy: 0.9472\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1367 - accuracy: 0.9418 - val_loss: 0.1386 - val_accuracy: 0.9503\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1369 - accuracy: 0.9403 - val_loss: 0.1378 - val_accuracy: 0.9503\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1364 - accuracy: 0.9372 - val_loss: 0.1366 - val_accuracy: 0.9472\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1352 - accuracy: 0.9387 - val_loss: 0.1360 - val_accuracy: 0.9503\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1349 - accuracy: 0.9387 - val_loss: 0.1366 - val_accuracy: 0.9503\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1343 - accuracy: 0.9418 - val_loss: 0.1380 - val_accuracy: 0.9472\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1354 - accuracy: 0.9403 - val_loss: 0.1349 - val_accuracy: 0.9503\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1329 - accuracy: 0.9418 - val_loss: 0.1333 - val_accuracy: 0.9503\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1338 - accuracy: 0.9403 - val_loss: 0.1322 - val_accuracy: 0.9503\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1319 - accuracy: 0.9418 - val_loss: 0.1328 - val_accuracy: 0.9472\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1326 - accuracy: 0.9433 - val_loss: 0.1341 - val_accuracy: 0.9472\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1327 - accuracy: 0.9403 - val_loss: 0.1291 - val_accuracy: 0.9503\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1292 - accuracy: 0.9449 - val_loss: 0.1304 - val_accuracy: 0.9503\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1358 - accuracy: 0.9418 - val_loss: 0.1275 - val_accuracy: 0.9503\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1312 - accuracy: 0.9403 - val_loss: 0.1295 - val_accuracy: 0.9472\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1315 - accuracy: 0.9403 - val_loss: 0.1328 - val_accuracy: 0.9503\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1343 - accuracy: 0.9418 - val_loss: 0.1263 - val_accuracy: 0.9503\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1293 - accuracy: 0.9433 - val_loss: 0.1257 - val_accuracy: 0.9503\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1317 - accuracy: 0.9418 - val_loss: 0.1240 - val_accuracy: 0.9472\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1280 - accuracy: 0.9418 - val_loss: 0.1265 - val_accuracy: 0.9503\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1302 - accuracy: 0.9433 - val_loss: 0.1262 - val_accuracy: 0.9503\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1305 - accuracy: 0.9403 - val_loss: 0.1231 - val_accuracy: 0.9503\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1266 - accuracy: 0.9403 - val_loss: 0.1236 - val_accuracy: 0.9503\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1261 - accuracy: 0.9418 - val_loss: 0.1256 - val_accuracy: 0.9503\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1271 - accuracy: 0.9433 - val_loss: 0.1252 - val_accuracy: 0.9503\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1262 - accuracy: 0.9418 - val_loss: 0.1234 - val_accuracy: 0.9503\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1249 - accuracy: 0.9403 - val_loss: 0.1232 - val_accuracy: 0.9503\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1256 - accuracy: 0.9418 - val_loss: 0.1233 - val_accuracy: 0.9503\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1242 - accuracy: 0.9464 - val_loss: 0.1242 - val_accuracy: 0.9503\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1241 - accuracy: 0.9464 - val_loss: 0.1238 - val_accuracy: 0.9503\n",
      "Epoch 291/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1240 - accuracy: 0.9464 - val_loss: 0.1222 - val_accuracy: 0.9503\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1230 - accuracy: 0.9464 - val_loss: 0.1212 - val_accuracy: 0.9503\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1226 - accuracy: 0.9495 - val_loss: 0.1204 - val_accuracy: 0.9534\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1230 - accuracy: 0.9479 - val_loss: 0.1201 - val_accuracy: 0.9503\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1235 - accuracy: 0.9449 - val_loss: 0.1207 - val_accuracy: 0.9503\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1214 - accuracy: 0.9464 - val_loss: 0.1191 - val_accuracy: 0.9534\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1221 - accuracy: 0.9464 - val_loss: 0.1196 - val_accuracy: 0.9534\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1229 - accuracy: 0.9449 - val_loss: 0.1193 - val_accuracy: 0.9503\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1204 - accuracy: 0.9464 - val_loss: 0.1247 - val_accuracy: 0.9503\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1242 - accuracy: 0.9433 - val_loss: 0.1208 - val_accuracy: 0.9503\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1192 - accuracy: 0.9464 - val_loss: 0.1208 - val_accuracy: 0.9534\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1249 - accuracy: 0.9525 - val_loss: 0.1187 - val_accuracy: 0.9534\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1204 - accuracy: 0.9449 - val_loss: 0.1238 - val_accuracy: 0.9503\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1221 - accuracy: 0.9433 - val_loss: 0.1225 - val_accuracy: 0.9503\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1208 - accuracy: 0.9433 - val_loss: 0.1177 - val_accuracy: 0.9472\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1177 - accuracy: 0.9510 - val_loss: 0.1174 - val_accuracy: 0.9534\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1181 - accuracy: 0.9525 - val_loss: 0.1178 - val_accuracy: 0.9503\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1165 - accuracy: 0.9479 - val_loss: 0.1209 - val_accuracy: 0.9472\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1187 - accuracy: 0.9449 - val_loss: 0.1185 - val_accuracy: 0.9472\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1175 - accuracy: 0.9479 - val_loss: 0.1161 - val_accuracy: 0.9534\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1169 - accuracy: 0.9525 - val_loss: 0.1165 - val_accuracy: 0.9503\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1152 - accuracy: 0.9479 - val_loss: 0.1199 - val_accuracy: 0.9472\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1166 - accuracy: 0.9449 - val_loss: 0.1187 - val_accuracy: 0.9472\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1163 - accuracy: 0.9449 - val_loss: 0.1164 - val_accuracy: 0.9472\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1140 - accuracy: 0.9479 - val_loss: 0.1163 - val_accuracy: 0.9472\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1138 - accuracy: 0.9479 - val_loss: 0.1146 - val_accuracy: 0.9472\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1136 - accuracy: 0.9495 - val_loss: 0.1129 - val_accuracy: 0.9503\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1131 - accuracy: 0.9510 - val_loss: 0.1126 - val_accuracy: 0.9503\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1124 - accuracy: 0.9510 - val_loss: 0.1121 - val_accuracy: 0.9534\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1123 - accuracy: 0.9510 - val_loss: 0.1119 - val_accuracy: 0.9534\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1122 - accuracy: 0.9510 - val_loss: 0.1118 - val_accuracy: 0.9534\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1118 - accuracy: 0.9525 - val_loss: 0.1115 - val_accuracy: 0.9534\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1112 - accuracy: 0.9525 - val_loss: 0.1117 - val_accuracy: 0.9472\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1109 - accuracy: 0.9495 - val_loss: 0.1110 - val_accuracy: 0.9472\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1106 - accuracy: 0.9510 - val_loss: 0.1097 - val_accuracy: 0.9534\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1110 - accuracy: 0.9541 - val_loss: 0.1097 - val_accuracy: 0.9534\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1098 - accuracy: 0.9541 - val_loss: 0.1135 - val_accuracy: 0.9472\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1107 - accuracy: 0.9464 - val_loss: 0.1154 - val_accuracy: 0.9472\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1105 - accuracy: 0.9449 - val_loss: 0.1097 - val_accuracy: 0.9534\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1105 - accuracy: 0.9587 - val_loss: 0.1093 - val_accuracy: 0.9565\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1110 - accuracy: 0.9556 - val_loss: 0.1105 - val_accuracy: 0.9503\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1082 - accuracy: 0.9495 - val_loss: 0.1110 - val_accuracy: 0.9472\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1085 - accuracy: 0.9479 - val_loss: 0.1091 - val_accuracy: 0.9534\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1069 - accuracy: 0.9525 - val_loss: 0.1070 - val_accuracy: 0.9565\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1090 - accuracy: 0.9617 - val_loss: 0.1066 - val_accuracy: 0.9534\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1064 - accuracy: 0.9541 - val_loss: 0.1123 - val_accuracy: 0.9503\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1104 - accuracy: 0.9449 - val_loss: 0.1093 - val_accuracy: 0.9472\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1071 - accuracy: 0.9479 - val_loss: 0.1039 - val_accuracy: 0.9534\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1087 - accuracy: 0.9602 - val_loss: 0.1042 - val_accuracy: 0.9534\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1096 - accuracy: 0.9602 - val_loss: 0.1068 - val_accuracy: 0.9534\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1054 - accuracy: 0.9510 - val_loss: 0.1063 - val_accuracy: 0.9534\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1048 - accuracy: 0.9541 - val_loss: 0.1051 - val_accuracy: 0.9534\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1043 - accuracy: 0.9556 - val_loss: 0.1057 - val_accuracy: 0.9534\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1038 - accuracy: 0.9525 - val_loss: 0.1078 - val_accuracy: 0.9472\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1055 - accuracy: 0.9495 - val_loss: 0.1070 - val_accuracy: 0.9503\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1046 - accuracy: 0.9525 - val_loss: 0.1065 - val_accuracy: 0.9534\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1031 - accuracy: 0.9541 - val_loss: 0.1063 - val_accuracy: 0.9534\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1023 - accuracy: 0.9541 - val_loss: 0.1060 - val_accuracy: 0.9534\n",
      "Epoch 349/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1034 - accuracy: 0.9694 - val_loss: 0.1061 - val_accuracy: 0.9534\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1018 - accuracy: 0.9602 - val_loss: 0.1101 - val_accuracy: 0.9503\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1040 - accuracy: 0.9510 - val_loss: 0.1094 - val_accuracy: 0.9503\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1033 - accuracy: 0.9495 - val_loss: 0.1043 - val_accuracy: 0.9534\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1011 - accuracy: 0.9617 - val_loss: 0.1034 - val_accuracy: 0.9534\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1010 - accuracy: 0.9632 - val_loss: 0.1073 - val_accuracy: 0.9534\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1020 - accuracy: 0.9495 - val_loss: 0.1145 - val_accuracy: 0.9534\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1075 - accuracy: 0.9449 - val_loss: 0.1042 - val_accuracy: 0.9534\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1007 - accuracy: 0.9556 - val_loss: 0.1022 - val_accuracy: 0.9534\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1049 - accuracy: 0.9678 - val_loss: 0.1007 - val_accuracy: 0.9534\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1006 - accuracy: 0.9541 - val_loss: 0.1069 - val_accuracy: 0.9503\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1026 - accuracy: 0.9479 - val_loss: 0.1008 - val_accuracy: 0.9534\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1002 - accuracy: 0.9556 - val_loss: 0.0987 - val_accuracy: 0.9534\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0996 - accuracy: 0.9648 - val_loss: 0.0997 - val_accuracy: 0.9534\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0981 - accuracy: 0.9602 - val_loss: 0.1017 - val_accuracy: 0.9534\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0976 - accuracy: 0.9541 - val_loss: 0.1058 - val_accuracy: 0.9534\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0998 - accuracy: 0.9495 - val_loss: 0.1031 - val_accuracy: 0.9534\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0961 - accuracy: 0.9617 - val_loss: 0.1024 - val_accuracy: 0.9565\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1038 - accuracy: 0.9648 - val_loss: 0.1016 - val_accuracy: 0.9534\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0984 - accuracy: 0.9617 - val_loss: 0.1098 - val_accuracy: 0.9534\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1009 - accuracy: 0.9495 - val_loss: 0.1035 - val_accuracy: 0.9534\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0968 - accuracy: 0.9541 - val_loss: 0.0990 - val_accuracy: 0.9565\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0984 - accuracy: 0.9678 - val_loss: 0.0979 - val_accuracy: 0.9565\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0960 - accuracy: 0.9694 - val_loss: 0.0991 - val_accuracy: 0.9534\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0958 - accuracy: 0.9571 - val_loss: 0.0997 - val_accuracy: 0.9534\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0964 - accuracy: 0.9556 - val_loss: 0.0961 - val_accuracy: 0.9565\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0947 - accuracy: 0.9694 - val_loss: 0.0954 - val_accuracy: 0.9565\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0941 - accuracy: 0.9678 - val_loss: 0.0988 - val_accuracy: 0.9534\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0948 - accuracy: 0.9541 - val_loss: 0.1024 - val_accuracy: 0.9565\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0967 - accuracy: 0.9541 - val_loss: 0.0984 - val_accuracy: 0.9534\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0939 - accuracy: 0.9587 - val_loss: 0.0964 - val_accuracy: 0.9565\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0946 - accuracy: 0.9678 - val_loss: 0.0968 - val_accuracy: 0.9565\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0946 - accuracy: 0.9678 - val_loss: 0.0967 - val_accuracy: 0.9565\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0940 - accuracy: 0.9694 - val_loss: 0.0975 - val_accuracy: 0.9534\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0931 - accuracy: 0.9648 - val_loss: 0.1009 - val_accuracy: 0.9596\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0940 - accuracy: 0.9525 - val_loss: 0.0956 - val_accuracy: 0.9534\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0913 - accuracy: 0.9678 - val_loss: 0.0941 - val_accuracy: 0.9627\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0972 - accuracy: 0.9663 - val_loss: 0.0934 - val_accuracy: 0.9565\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0955 - accuracy: 0.9663 - val_loss: 0.0969 - val_accuracy: 0.9596\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0925 - accuracy: 0.9571 - val_loss: 0.0974 - val_accuracy: 0.9596\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0919 - accuracy: 0.9571 - val_loss: 0.0940 - val_accuracy: 0.9565\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0912 - accuracy: 0.9709 - val_loss: 0.0939 - val_accuracy: 0.9596\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0915 - accuracy: 0.9694 - val_loss: 0.0973 - val_accuracy: 0.9565\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0921 - accuracy: 0.9648 - val_loss: 0.0987 - val_accuracy: 0.9565\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0911 - accuracy: 0.9678 - val_loss: 0.0941 - val_accuracy: 0.9596\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0908 - accuracy: 0.9694 - val_loss: 0.0932 - val_accuracy: 0.9596\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0910 - accuracy: 0.9694 - val_loss: 0.0944 - val_accuracy: 0.9565\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0892 - accuracy: 0.9678 - val_loss: 0.0948 - val_accuracy: 0.9565\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0894 - accuracy: 0.9632 - val_loss: 0.0909 - val_accuracy: 0.9565\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0885 - accuracy: 0.9724 - val_loss: 0.0891 - val_accuracy: 0.9627\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0906 - accuracy: 0.9678 - val_loss: 0.0904 - val_accuracy: 0.9565\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0875 - accuracy: 0.9678 - val_loss: 0.0988 - val_accuracy: 0.9627\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0931 - accuracy: 0.9541 - val_loss: 0.0965 - val_accuracy: 0.9596\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0905 - accuracy: 0.9587 - val_loss: 0.0900 - val_accuracy: 0.9596\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0872 - accuracy: 0.9709 - val_loss: 0.0896 - val_accuracy: 0.9689\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0907 - accuracy: 0.9663 - val_loss: 0.0895 - val_accuracy: 0.9596\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0871 - accuracy: 0.9663 - val_loss: 0.0946 - val_accuracy: 0.9596\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0881 - accuracy: 0.9632 - val_loss: 0.0914 - val_accuracy: 0.9565\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0883 - accuracy: 0.9663 - val_loss: 0.0896 - val_accuracy: 0.9565\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0864 - accuracy: 0.9694 - val_loss: 0.0909 - val_accuracy: 0.9565\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0858 - accuracy: 0.9678 - val_loss: 0.0886 - val_accuracy: 0.9565\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0871 - accuracy: 0.9709 - val_loss: 0.0882 - val_accuracy: 0.9596\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0873 - accuracy: 0.9724 - val_loss: 0.0922 - val_accuracy: 0.9627\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0859 - accuracy: 0.9632 - val_loss: 0.0987 - val_accuracy: 0.9596\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0907 - accuracy: 0.9510 - val_loss: 0.0945 - val_accuracy: 0.9627\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0881 - accuracy: 0.9602 - val_loss: 0.0885 - val_accuracy: 0.9658\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0867 - accuracy: 0.9694 - val_loss: 0.0890 - val_accuracy: 0.9596\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0846 - accuracy: 0.9709 - val_loss: 0.0938 - val_accuracy: 0.9627\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0860 - accuracy: 0.9632 - val_loss: 0.0973 - val_accuracy: 0.9596\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0869 - accuracy: 0.9617 - val_loss: 0.0890 - val_accuracy: 0.9565\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0831 - accuracy: 0.9724 - val_loss: 0.0883 - val_accuracy: 0.9658\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0899 - accuracy: 0.9648 - val_loss: 0.0880 - val_accuracy: 0.9565\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0846 - accuracy: 0.9709 - val_loss: 0.0953 - val_accuracy: 0.9596\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0863 - accuracy: 0.9632 - val_loss: 0.0893 - val_accuracy: 0.9565\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0826 - accuracy: 0.9709 - val_loss: 0.0863 - val_accuracy: 0.9720\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0847 - accuracy: 0.9709 - val_loss: 0.0869 - val_accuracy: 0.9658\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0839 - accuracy: 0.9678 - val_loss: 0.0893 - val_accuracy: 0.9596\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0825 - accuracy: 0.9694 - val_loss: 0.0861 - val_accuracy: 0.9658\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0820 - accuracy: 0.9709 - val_loss: 0.0864 - val_accuracy: 0.9596\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0819 - accuracy: 0.9694 - val_loss: 0.0873 - val_accuracy: 0.9596\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0817 - accuracy: 0.9694 - val_loss: 0.0854 - val_accuracy: 0.9658\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0813 - accuracy: 0.9724 - val_loss: 0.0854 - val_accuracy: 0.9658\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0813 - accuracy: 0.9709 - val_loss: 0.0875 - val_accuracy: 0.9596\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0808 - accuracy: 0.9709 - val_loss: 0.0920 - val_accuracy: 0.9627\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0832 - accuracy: 0.9663 - val_loss: 0.0887 - val_accuracy: 0.9596\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0818 - accuracy: 0.9694 - val_loss: 0.0853 - val_accuracy: 0.9627\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0801 - accuracy: 0.9724 - val_loss: 0.0854 - val_accuracy: 0.9596\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0805 - accuracy: 0.9724 - val_loss: 0.0829 - val_accuracy: 0.9627\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0806 - accuracy: 0.9724 - val_loss: 0.0819 - val_accuracy: 0.9658\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0804 - accuracy: 0.9694 - val_loss: 0.0875 - val_accuracy: 0.9627\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0811 - accuracy: 0.9678 - val_loss: 0.0931 - val_accuracy: 0.9596\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0846 - accuracy: 0.9648 - val_loss: 0.0845 - val_accuracy: 0.9627\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0787 - accuracy: 0.9709 - val_loss: 0.0809 - val_accuracy: 0.9720\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0816 - accuracy: 0.9709 - val_loss: 0.0800 - val_accuracy: 0.9720\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0797 - accuracy: 0.9694 - val_loss: 0.0853 - val_accuracy: 0.9627\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0798 - accuracy: 0.9678 - val_loss: 0.0877 - val_accuracy: 0.9627\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0809 - accuracy: 0.9648 - val_loss: 0.0809 - val_accuracy: 0.9658\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0790 - accuracy: 0.9740 - val_loss: 0.0795 - val_accuracy: 0.9720\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0800 - accuracy: 0.9724 - val_loss: 0.0825 - val_accuracy: 0.9658\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0779 - accuracy: 0.9709 - val_loss: 0.0848 - val_accuracy: 0.9627\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0780 - accuracy: 0.9694 - val_loss: 0.0809 - val_accuracy: 0.9658\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0781 - accuracy: 0.9740 - val_loss: 0.0797 - val_accuracy: 0.9658\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.0845 - val_accuracy: 0.9627\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0797 - accuracy: 0.9678 - val_loss: 0.0826 - val_accuracy: 0.9658\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0770 - accuracy: 0.9709 - val_loss: 0.0775 - val_accuracy: 0.9720\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0811 - accuracy: 0.9694 - val_loss: 0.0794 - val_accuracy: 0.9658\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0757 - accuracy: 0.9724 - val_loss: 0.0938 - val_accuracy: 0.9596\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0857 - accuracy: 0.9617 - val_loss: 0.0889 - val_accuracy: 0.9596\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0825 - accuracy: 0.9663 - val_loss: 0.0781 - val_accuracy: 0.9720\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0763 - accuracy: 0.9709 - val_loss: 0.0772 - val_accuracy: 0.9720\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0764 - accuracy: 0.9709 - val_loss: 0.0804 - val_accuracy: 0.9720\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0761 - accuracy: 0.9694 - val_loss: 0.0836 - val_accuracy: 0.9596\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0769 - accuracy: 0.9694 - val_loss: 0.0785 - val_accuracy: 0.9658\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0760 - accuracy: 0.9694 - val_loss: 0.0774 - val_accuracy: 0.9720\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0757 - accuracy: 0.9709 - val_loss: 0.0832 - val_accuracy: 0.9658\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0753 - accuracy: 0.9709 - val_loss: 0.0890 - val_accuracy: 0.9596\n",
      "Epoch 465/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0779 - accuracy: 0.9663 - val_loss: 0.0834 - val_accuracy: 0.9658\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0745 - accuracy: 0.9724 - val_loss: 0.0804 - val_accuracy: 0.9689\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0758 - accuracy: 0.9709 - val_loss: 0.0820 - val_accuracy: 0.9627\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0737 - accuracy: 0.9740 - val_loss: 0.0889 - val_accuracy: 0.9627\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0775 - accuracy: 0.9663 - val_loss: 0.0853 - val_accuracy: 0.9627\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0751 - accuracy: 0.9709 - val_loss: 0.0795 - val_accuracy: 0.9689\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0766 - accuracy: 0.9694 - val_loss: 0.0810 - val_accuracy: 0.9627\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0739 - accuracy: 0.9740 - val_loss: 0.0866 - val_accuracy: 0.9627\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0756 - accuracy: 0.9694 - val_loss: 0.0848 - val_accuracy: 0.9627\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0741 - accuracy: 0.9694 - val_loss: 0.0818 - val_accuracy: 0.9658\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0726 - accuracy: 0.9740 - val_loss: 0.0777 - val_accuracy: 0.9689\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0744 - accuracy: 0.9709 - val_loss: 0.0776 - val_accuracy: 0.9689\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0729 - accuracy: 0.9724 - val_loss: 0.0853 - val_accuracy: 0.9627\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0757 - accuracy: 0.9694 - val_loss: 0.0846 - val_accuracy: 0.9627\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0713 - accuracy: 0.9755 - val_loss: 0.0788 - val_accuracy: 0.9658\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0795 - accuracy: 0.9709 - val_loss: 0.0785 - val_accuracy: 0.9720\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0709 - accuracy: 0.9724 - val_loss: 0.0953 - val_accuracy: 0.9596\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.96 - 0s 7ms/step - loss: 0.0822 - accuracy: 0.9648 - val_loss: 0.0941 - val_accuracy: 0.9596\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0776 - accuracy: 0.9648 - val_loss: 0.0755 - val_accuracy: 0.9689\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0790 - accuracy: 0.9709 - val_loss: 0.0756 - val_accuracy: 0.9689\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0759 - accuracy: 0.9740 - val_loss: 0.0898 - val_accuracy: 0.9596\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0796 - accuracy: 0.9678 - val_loss: 0.0999 - val_accuracy: 0.9596\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0863 - accuracy: 0.9617 - val_loss: 0.0804 - val_accuracy: 0.9689\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0701 - accuracy: 0.9770 - val_loss: 0.0760 - val_accuracy: 0.9689\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.9709 - val_loss: 0.0761 - val_accuracy: 0.9689\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0709 - accuracy: 0.9740 - val_loss: 0.0862 - val_accuracy: 0.9658\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0737 - accuracy: 0.9709 - val_loss: 0.0906 - val_accuracy: 0.9627\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0754 - accuracy: 0.9678 - val_loss: 0.0795 - val_accuracy: 0.9689\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0712 - accuracy: 0.9724 - val_loss: 0.0775 - val_accuracy: 0.9658\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0734 - accuracy: 0.9740 - val_loss: 0.0792 - val_accuracy: 0.9720\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0690 - accuracy: 0.9740 - val_loss: 0.0816 - val_accuracy: 0.9689\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0694 - accuracy: 0.9740 - val_loss: 0.0829 - val_accuracy: 0.9689\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0699 - accuracy: 0.9740 - val_loss: 0.0793 - val_accuracy: 0.9720\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0691 - accuracy: 0.9709 - val_loss: 0.0769 - val_accuracy: 0.9720\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0690 - accuracy: 0.9709 - val_loss: 0.0792 - val_accuracy: 0.9720\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0689 - accuracy: 0.9740 - val_loss: 0.0810 - val_accuracy: 0.9689\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#학습과 검증을 동시에 하는 것\n",
    "#그래프로 확인하기\n",
    "\n",
    "import numpy\n",
    "\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "#df=df_pre.sample(frac=0.15)\n",
    "dataset=df.values\n",
    "X=dataset[:,0:12].astype(float)\n",
    "Y=dataset[:,12]\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(30,input_dim=12,activation='relu'))\n",
    "model.add(Dense(12,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "#모델 실행 및 저장\n",
    "history=model.fit(X,Y,validation_split=0.33, epochs=500, batch_size=500)\n",
    "\n",
    "#y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장\n",
    "y_vloss=history.history['val_loss']\n",
    "\n",
    "#y_acc에 학습셋으로 측정한 정확도의 값을 저장\n",
    "y_acc=history.history['accuracy']\n",
    "\n",
    "#x값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시\n",
    "x_len=numpy.arange(len(y_acc))\n",
    "plt.plot(x_len,y_vloss,'o',c='red',markersize=3)#테스트셋 오차\n",
    "plt.plot(x_len,y_acc,'o',c='blue',markersize=3)#학습셋 정확도\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#검증데이터(테스트셋 오차)는 어느 정도가 넘어가면 다시 커짐(과적합)\n",
    "#학습셋의 정확도는 시간이 흐를수록 좋아지지만 테스트 결과는 과적합이 되면 더 나아지지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
